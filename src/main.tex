\documentclass[a4paper, 11pt, oneside]{book}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}   % Babel, replace with your language
\usepackage[T1]{fontenc}      % Font encoding
\usepackage{lmodern,cmap}     % lmodern font, correctly copyable characters in pdf

\usepackage{hyperref}%\usepackage[hidelinks]{hyperref}
\usepackage{color}
\usepackage{setspace}
\usepackage{caption}

\usepackage{geometry}
\newgeometry{left=3.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\usepackage{amsmath}
\usepackage{amsfonts}

\makeatletter
\newif\if@restonecol
\makeatother

%ALGORITHM
\let\algorithm\relax
\let\endalgorithm\relax
\usepackage[linesnumbered,ruled,algonl,vlined]{algorithm2e}
\newcommand\mycommentfont[1]{\footnotesize \ttfamily \textcolor{blue}{#1}}
\SetCommentSty{mycommentfont}
\newcounter{algoline}

%GRAPHICS
\usepackage{graphicx}
\usepackage{epstopdf}
\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps}
\usepackage{placeins} % for \FloatbBarrier

%HEADER FOOTER
\usepackage{fancyhdr}
\fancyhf{}
\fancyhead[LO]{\slshape \rightmark}
\fancyhead[LE]{\slshape \leftmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.0mm}
\renewcommand{\footrulewidth}{0.0mm}
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}

\frenchspacing \sloppy

\newcommand*{\perm}[2]{{}^{#1}\!P_{#2}}%
\newcommand*{\comb}[2]{{}^{#1}C_{#2}}%

\newcommand{\eat}[1]{}
\hyphenation{}

\begin{document}

\title{Ensemble Learning in Data Streams}


\author{
Hossain Mahmud \\
Technische Universit\"at M\"unchen\\
mahmud@in.tum.de
}

\date{}

%\maketitle
%\pagenumbering{roman}
%\tableofcontents
%\clearpage
%
%\listoftables
%\addcontentsline{toc}{chapter}{List of Tables}
%\clearpage
%
%\listoffigures
%\addcontentsline{toc}{chapter}{List of Figures}
%\clearpage
%
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}
%\clearpage

%\normalsize
\onehalfspacing
\pagenumbering{arabic}
\setcounter{page}{1}

\input{literature.tex}

\chapter{Background}
\label{chp:background}
This chapter discusses the primitives of data and stream classification. First, a brief overview of traditional data classification methods are presented. Followed by a section on data stream classification where challenges and approaches for stream classification are introduced. Finally, overview of current state-of-the-art ensemble learning methods are discussed. These discussion are lays the foundation of the approach introduced in this thesis.

\section{Learning Algorithms}
\label{sec:bg:learningalgos}
The goal of data classification process is to predict an outcome based on some give data. In order to do so, data mining algorithms first processes a training set containing a set of attributes and corresponding outcome: a class or a value. Algorithms develop hypotheses that best describe the relationship among the attributes and the outcome for the total instance set.  Formally, learning problems are defined as follow: Given a data set of $m$ instances  $D \equiv \{ \{\vec{x}_1, y_1\}, \{\vec{x}_2, y_2\}, \{\vec{x}_3, y_3\}, \dots, \{\vec{x}_m, y_m\} \}$ where $ \vec{x} = \{x_1, x_2, x_3, \dots, x_n\}$, learning algorithms try to approximate $H \equiv y = f(\vec{x})$. Additional to being typical linear, polynomial, etc. functions, $H$ can also be a set of IF-THEN rules. These rules or functions are then used to predict unseen instances where outcome class is not known. These algorithms are known as learning algorithms, and in last few decades, have widely been researched in the field of machine learning. This section briefly discusses few of the basic algorithms upon which the foundation of ensemble methods and this thesis are laid. The detailed discussion of these algorithms are out of the scope of this chapter. Thus we discuss only the core concepts here.

\subsection{$k$-Nearest Neighbors}
$k$-nearest neighbor algorithm, in short $k$-NN, is one the simplest learning available. It is a non-parametric instance-based lazy learning algorithm and works for both supervised and unsupervised learning. It uses similarity measure like distance functions to classify unknown instances. The decision is a majority voting of  $k$ neighbors where $k$ is predefined. For example, if $k= 1$ then an unknown instance is assigned the class label of its nearest neighbor. There are several schemes to find the $k$ nearest neighbors. For continuous data, some well-known distance functions are Euclidean distance, Manhattan distance, Minkowski distance, etc.
\begin{itemize}
    \item Euclidean distance $D_E = \sqrt{\sum_{\i =1 }^{k} (x_i - y_i)^2}$
    \item Manhattan distance $D_M = \sum_{\i =1 }^{k} |(x_i - y_i)|$
    \item Minkowski distance $D_{Mink}\sqrt[q]{\sum_{\i =1 }^{k} (|x_i - y_i|)^q}$
\end{itemize}
For categorical data, Hamming distance can be used. Hamming distance is defined as $D_H = \sum_{\i =1 }^{k} |(x_i - y_i)|$ where $|x_i - y_i|$ is $0$ if $x=y$, $1$ otherwise.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=3.0in]{figs/knn.jpg}
        \caption{Concept of $k$-NN}
        \label{fig:bg:knn}
    \end{center}
\end{figure}

Choosing an optimal $k$ is the prime challenge of $k$-NN algorithm, which is extremely training data dependent. Changing position of few training points could lead to a significant loss in performance. The method is particularly not stable in the class boundary. $k$ should be large enough that $k$-NN could overcome the noises in data, and small enough that instances of other classes are not included. Generally, higher $k$ reduces the overall noise and should be more precise. For most data set a value between $3$-$10$ performs much better than $1$-NN. Typically cross-validation is used to determine a good $k$. 

\subsection{Na\"ive Bayes}
Naive Bayes classification is based on Bayes rule. Bayes rule says that the probability of event $x$ conditioned on knowing event $y$, i.e. the probability of $x$ given $y$ is defined as
\[
    p(x|y) = \frac{p(x,y)}{p(y)} = \frac{p(y|x) p(x)}{p(y)}
\]
The naive Bayes classifier~\cite{langley92:nb} assumes that all the explanatory variables are independent. Given a feature set $X = x_1, x_2, \dots, x_n$ of $n$ independent variables, naive Bayes assigns the instances to $k$ possible outcome or classes, $p(C_k | X)$. Using Bayes rule, the conditional probability can be decomposed as:
\[
    p(C_k |X) = \frac{p(C_k) p(X|C_k)}{p(X)}.
\]
In other words,
\[
    posterior = \frac{prior \times likelihood}{evidence}.
\]
Using chain rule repetitively, it can be expressed as follows:
\[
    p(C_k |X) = p(C_k) \prod_{i=1}^n p(x_i | C_k)
\]
The predicted class is the one which maximizes the conditional probabilities $p(C_k|X)$, that is, classifier assigns the class label $\hat{y} = C_k$ for some $k$ that satisfies:
\[
    \hat{y} = \underset{k \in \{1, \dots, K\}}{argmax}  p(C_k) \prod_{i=1}^n p(x_i | C_k)
\]

\subsection{Decision Tree}
Decision trees (DTs) are another popular genre of non-parametric supervised learning algorithms. Trees allow a way to graphically organize a sequential decision process. Decision tree, a directed acyclic graph, contains decision nodes, each with branches for all alternate decisions. Leaf nodes are labeled with respective class label. Each path from root to leaf is a decision rule, consisting of conditional part (unions of all internal nodes' conditions) and a decision (of leaf node).

Decision trees use {\it information gain} to select the splitting attribute that would maximize the total entropy of each of the subtrees resulting from the spit. Entropy is a measurement of purity of an attribute, typically ranged between 0 and 1. A pure attribute, attribute with definitive value-class relationship, would have a low entropy and is easy to predict. On the other hand attribute with highly mixed value-class relationship would yield high entropy. A common entropy function is-
\[
    entropy = - \sum_{i=1}^n p_i \lg p_i.
\]
where $p_1, p_2, \dots, p_n$ are the distributions of several class attributes and $\sum p_i = 1$. Information gain is the difference between old and new entropy after a split. This is good measurement of relevance of an attribute. However, in cases where an attribute can take on a large number of distinct values, information gain is not ideal. Presence of large set of values could uniquely identify the classes but also reduces chance to generalize unseen instance and should be avoided to be place near root.

Decision tree is, however, computationally feasible. Average cost of constructing a decision tree for $n$ instances with $m$ attributes is $O(mn \lg n)$, and querying time is $O(\lg n)$.

\subsection{Neural Network}
The Neural Network (NN) is a learning method inspired by biological neural network. A set of inter-connected nodes (also known as perceptrons and neurons) mimics biological neural network. Figure~\ref{fig:bg:nnlayer} shows a skeleton of a neural network. Neural network is a weighted directed graph where a input layer is connected to the output layer via some layers of hidden layers. This is commonly known as Multi-Layer Perceptron (MLP). Decisions are made looking only into the output layer only. Hidden layers enlarge the space of hypotheses. If there is no hidden layer, then neural network becomes a simple regression problem i.e. output is a linear function of inputs. Each connection among these nodes has an associated weight. Given an input data set, these weights are updated accordingly to best fit the classification model. Learning is done by back-propagation algorithm~\cite{raul96:nn}. Errors are propagated back from the output layer to the hidden layer and weights are updated to minimize error.
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=4.0cm]{figs/nnlayers.png}
        \caption{Neural network layers}
        \label{fig:bg:nnlayer}
    \end{center}
\end{figure}

Learning starts with assigning a small value as initial weights to all the connection weights. In case of a single perceptron, learning process is analogous to moving a parametric hyperplane around. Let $w_i$ be the weight of $i$-th input, then after $t+1$ iteration weight $w_i(t+1) = w_i(t) + \nabla E_i(t)$, where $\nabla E_i$ is the gradient of the error function. For a input vector $\mathbf{x}$ and true output $y$, $E$ is defined as the squared error:
\[
    E = \frac{1}{2} Err^2 = \frac{1}{2} (y - h_w(\mathbf{x}))^2
\]
Thus gradient of $E$ would be:
\begin{align*}
    \nabla E = \frac{\partial E}{\partial W_j} & = Err \times \frac{\partial Err}{\partial W_j} = Err \times \frac{\partial}{\partial W_j} (y - g(\sum_{j=0}^n w_j x_j)) \\
    & = - Err \times g^{\prime} (in) \times x_j
\end{align*}
Based on this, weight update rule would be:
\[
    w_j = w_j + \alpha \times Err \times g^{\prime} (in) \times x_j
\]
where $\alpha$ is learning rate coefficient. Back propagation algorithm for multi-layer perceptron also works in similar fashion. Weights in the output layer are updated using following equation:
\[
    w_{j,i} = w_{j,i} + \alpha \times a_j \times \nabla_i
\]
where $\nabla_i = Err \times g^\prime(in_i)$. Hidden layers propagates this error from the output layer using:
\begin{align*}
    \nabla_j &= g^\prime (in_i) \sum_i w_{j,i} \nabla_i \\
    w_{k, j} &= w_{k, j} + \alpha \times a_k \times \nabla_j
\end{align*}

MLP is a very expressive method. Only $1$ hidden layer is sufficient to represent all continuous functions and $2$ can represent any functions. MLP is prone to local minima, which is avoided by running MLP multiple times with different initial weight settings.

\section{Data Stream Classification}
Traditional data mining algorithms work in a memory bounded environment and require multiple scan of the training data. In stream environment, one of the major assumptions is that new data samples are introduced in the system with such a high rate that repetitive analysis becomes infeasible. Thus, for stream classification, algorithms should be able to look into a instance only once and decide upon that. A bounded memory buffer can be used to facilitate some level of repetition. However, which instances are to remember and which are to forget would then become a decision choice. An alternate choice is to maintain sufficient statistics to have a representation of the data. The process of deletion or summarization of instances, however, means that some information are being lost over the time.

In this section, these challenges are first discussed in details. Then it presents the basis of some of the concepts arisen to handle these challenges. Finally, before moving onto the ensemble leaning, it discusses current state-of-the-art algorithms for stream mining.

\subsection{Challenges}
Challenges posed by the streaming environment can be categorized into two groups: (i) relating to runtime and memory requirements and (ii) relating to underlying concept identification. Speed of incoming data, unbounded memory requirement, single-pass learning fall into the first category. On the other hand, lack of labeled data, concept drift, evolution and recurrence are examples of latter category.

\paragraph{Speed of data arrival:}
As mentioned before, it is an inherent characteristic of data streams that it arrives with a high speed. The algorithm should be able to adapt to the high speed nature of streaming information. The rate of building the classifier model should be higher than the data rate. This gives a very limited amount of available time for classification as compared to the traditional batch classification models.

\paragraph{Memory requirements:}
To apply traditional batched approaches in streaming data, an unbounded memory would be needed. This challenge has been addressed using load shedding, sampling, aggregation, etc. Rather than storing all the instances, algorithms store a subset of the data set or some statistical values or a combination of both which represents the data seen thus far. New instances can be classified only by looking into these stored information. 

\paragraph{Single-pass learning:}
The premise of this requirement is two-fold. First, as mentioned above, data would not be available in the memory after a short period of time due to the volume of data. Secondly, even if the data remain available, running a batched-like approach for millions of data points would highly increase the running time of the algorithm. To attain faster processing time with limited storage, algorithm should access the data stream only once, or a small number of times. Mining models must posses the capability to learn the underlying nature of data in a single pass over the data.

\paragraph{Lack of labeled data:}
Unlike most data sets or settings of batched approaches, steam mining data sets are often poorly labeled. A large number of experimentations are done with generated data set where the data generation process can easily be controlled to have proper labeling. However, in data set collected from real world are often lack this. For example, to setup a supervised learning experimentation using a data set collected from social media, e.g. Twitter, data needs to be first categorized by human intervention. Manual labeling of such data is often costly, both in terms of resources and time. In practice, only a small fraction of data is labeled by human experts or automated scripts. A stream classification algorithm is thus required to be able to predict after observing a small number of instances, i.e. to be ready to predict anytime.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \resizebox{60mm}{!}{\includegraphics{figs/condrift-a.jpg}} &
            \resizebox{60mm}{!}{\includegraphics{figs/condrift-b.jpg}} \\
            \scriptsize{(a)\hspace{0mm}} & \scriptsize{(b)}    
        \end{tabular}
    \caption{Concept drift in data streams.}
    \label{fig:bg:condrift}
    \end{center}
\end{figure}
\paragraph{Concept drift:}
Concept drift is a statistical property of data streams where the target variable drifts away from the model that is trying to predict it. In other words the underlying data distribution is changing over time. As a result accuracy of the classifier model decreases over time. For example, buying pattern of the customers in a store changes over time, mostly due to the seasonality. Electrical and mechanical devices wear off over time, producing shifted result which would cause drift in the observing data. Learning models should adapt to these changes quickly and accurately. Let us consider the example in Figure~\ref{fig:bg:condrift}. As a new chunk arrives (Figure~\ref{fig:bg:condrift}a), a new classifier is learned. The decision boundary is denoted by the straight line. The positive examples are represented by unfilled circles while the negative examples are represented by filled circles. With time the concept of some of the examples may change. As shown in Figure~\ref{fig:bg:condrift}b, due to concept drift some negative examples may have become positive. So, the previous decision boundary has become outdated and a new model has to be learned. Formally, concept drift is the change in the joint probability \(P(X, y) = P(y|X) \times P(X)\). Thus, observing the change in $y$ for given $X$, i.e. $P(y|X)$ is the key for detection.

One challenge posed here is to differentiate the noise in data and the actual shift of the concept. Often in streaming environment data contains the both. Rate of drift is also a factor in detection. Sudden drift, known as {\it concept shift}, is easier to detect than concept drift, which is considered to be gradual.

\begin{figure}[htbp]
    \begin{center}
        \begin{tabular}{cc}
            \resizebox{60mm}{!}{\includegraphics{figs/conevol-a.jpg}} &
            \resizebox{60mm}{!}{\includegraphics{figs/conevol-b.jpg}} \\
            \scriptsize{(a)\hspace{0mm}} & \scriptsize{(b)}    
        \end{tabular}
        \caption{Concept evolution in data streams.}
        \label{fig:bg:dataevol}
    \end{center}
\end{figure}
\paragraph{Concept evolution:}
Concept evolution is referred to the emergence of a new class or a set of classes in stream data as the time flows. Twitter stream is an ideal example where concept evolution is very easily identifiable. Twitter reacts, seemingly, very fast upon important news around the globe. Looking into the different hash tag usages in such social media currently trending topics can be identified. To present a clearer picture let's consider following example in Figure~\ref{fig:bg:dataevol}. At certain point of time four classes and their corresponding decision boundaries are shown in the Figure~ (\ref{fig:bg:dataevol}a). With the more incoming data a novel class emerges, and for that decision boundaries need updating. Emergence of new class can affect any number of decision boundary/ rule, from one to all.

Concept evolution is also prone to noise. Furthermore, clear distinction between drift and evolution might not always be possible, partially due the lack of unlabeled data.

\paragraph{Class recurrence:}
Class recurrence is a special case of concept drift and evolution. A this case, the model forgets a class due the drift, however, later the class reappears (evolution) from the stream. Seasonality could be one cause of this situation. A intrusion in network traffic may reappear after a long time. Forgetting the earlier intrusions are not desired in such case. A fast recognition of the previously seen classes are desired in mining streams.

Following sections discuss the potential solutions to these challenges. First, how to address the limited resources and then the change detection schemes.

\subsection{Maintaining Sufficient Statistics}
In statistical evaluation, a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than does the statistic, as to which of those probability distributions is that of the population from which the sample was taken~\cite{fisher22:suffstat}. Mathematically, given a set  $\mathbf{X}$ of independent identically distributed data conditioned on an unknown parameter $\theta$, a sufficient statistic is a function $T(\mathbf{X})$ whose value contains all the information needed to compute any estimate of the parameter (e.g. maximum likelihood estimate). Using the factorization theorem (Theorem~\ref{thm:factor}), for a sufficient statistic $T(\mathbf{X})$, the joint distribution can be written as $ p(\mathbf{X}) = h(\mathbf{X}) g(\theta, T(\mathbf{X}))$. From this factorization, it can easily be seen that the maximum likelihood estimate of $\theta$ will interact with $\mathbf{X}$ only through $T(\mathbf{X})$. Typically, the sufficient statistic is a set of function or random variables of the data.

\begin{theorem}[Factorization Theorem]
    \label{thm:factor}
    Let $X_1, X_2, \dots , X_n$ be a random sample with joint density $f(x_1, x_2, \dots , x_n| \theta)$. A statistic $T = r(X_1, X_2, \dots , X_n)$ is sufficient if and only if the joint density can be factored as follows:
    \[
    f(x_1, x_2, \dots , x_n| \theta) = u(x_1, x_2, \dots , x_n) v(r(x_1, x_2, \dots , x_n), \theta)
    \]
    where $u$ and $v$ are non-negative functions. The function $u$ can depend on the full random sample $x_1, x_2, \dots , x_n$, but not on the unknown parameter $\theta$. The function $v$ can depend on $\theta$, but can depend on the random sample only through the value of $r(x_1, x_2, \dots , x_n)$.
\end{theorem}

\subsubsection{Bounds of Random Variable}
A random variable is a variable that can take a set or range of values, each with an associated probability, and are subjected to change due to the alteration or randomness of the data. Random variables are of two types: (i) discrete, and (ii) continuous. Discrete random variable take a set of possible values (e.g. outcome of coin flipping), but a continuous random variable can take any value within a range (e.g. age of people in a randomly sampled group).

A function that is used to estimating a random variable is called an estimator. Estimator function is dependent on the observable sample data, and used for estimating unknown population within an interval with certain degree of confidence. For an interval of the true value of the parameter associates with a confidence of $1 - \delta$, interval can be defined as follows:

\begin{itemize}    
    \item Absolute approximation: $\bar{X} - \epsilon \le \mu \le \bar{X} + \epsilon$, where $\epsilon$ is the absolute error.
    \item Relative approximation: $(1 - \delta)\bar{X} \le \mu \le (1 + \delta)\bar{X}$, where  is the relative error.
\end{itemize}

where $\mu$ and $\bar{X}$ represent actual and estimated mean. There are a number of theorems that provide bounds on the estimation, Chebyshev, Chernoff, Hoeffding bounds~\cite{hoeffding63:bound}, etc. are few of them.

\begin{theorem}[Chebyshev Bound]
\label{thm:chebyshev}
    Let $X$ be a random variable with standard deviation $\sigma$, the probability that the outcome of $X$ is no less than $k\sigma$ away from its mean is no more that $1/k^2$:
    \[
        P(|X-\mu| \le k\sigma) \le \frac{1}{k^2}
    \]
    In other words, it states that no more that $1/4$ of the values are more than $2$ standard deviation away, no more than $1/9$ are more than $3$ standard deviation away, and so on.
\end{theorem}

\begin{theorem}[Chernoff Bound]
\label{thm:chernoff}
    Let $X_1,X_2,\dots, X_n$ be independent random variables from Bernoulli experiments. Assuming that $P(X_i = 1) = p_i$. Let $X_s = P_n \sum_{i=1}{n} X_i$ be a random variable with expected value $\mu_s = P \sum_{i=1} np_i$. Then for any $\delta > 0$:
    \[
        P[X_s > (1+\delta) \mu_s] \le (\frac{e^\delta}{(1 + \delta)^{1 +\delta}} )  ^{\mu_s}
    \]
    and the absolute error is:
    \[
        \epsilon \le \sqrt{\frac{3 \bar{\mu}}{n} \ln (2/\delta)}
    \]
\end{theorem}

\begin{theorem}[Hoeffding Bound]
\label{thm:hoeffding}
    Let $X_1,X_2,\dots, X_n$ be independent random variables. Assuming that each $x_i$ is bounded, that is $P(X_i \in R = [a_i, b_i]) = 1$. Let $S = 1/n \sum_{i=1}{n} X_i$ whose expected value is $E[S]$. Then, for any $\epsilon > 0$:
    \[
        P[S - E[S] > \epsilon] \le e^{ \frac{2 n^2 \epsilon^2}{R^2} }
    \]
    and the absolute error is:
    \[
        \epsilon \le \sqrt{\frac{R^2 \ln(2/\delta)}{2n}}
    \]
\end{theorem}

Chernoff and Hoeffding bounds are independent of the underlying distribution of examples. They are more restrictive or conservative, and requires more observations as compared to the distribution dependent bounds. Chernoff bound is multiplicative and Hoeffding is additive. They are expressed as relative and absolute approximation, respectively.

These methods only take a finite number of values or a range. One of the well-known methods supports infinity is Poisson process. A random variable $x$ is Poisson random variable with parameter $\lambda$ if $x$ takes values $0,1,2, \dots, \infty$ with:
    \begin{equation}
    \label{eqn:poisson}
        p_k = P(x=k) = e^{-\lambda} \frac{\lambda^k}{k!}
    \end{equation}
where, $\lambda$ is both mean and variance, i.e. $E(X) = Var(X) = \lambda$
    
\subsubsection{Recursive Mean, Variance, and Correlation}
Fundamental equation of mean, variance, etc. are not usable for streams as past data points are lost as the time passes. However, their recursive version can easily be derived. Equation~\ref{eqn:mean}, \ref{eqn:var}, and \ref{eqn:corr} can be used to recursively compute mean, variance, and correlation respectively.

\begin{equation}
\label{eqn:mean}
    \bar{x}_i = \frac{(i-1) \times \bar{x}_{i-1} + x_i}{i}
\end{equation}

\begin{equation}
\label{eqn:var}
    \sigma_i = \sqrt{ \frac{\sum x_i^2 - \frac{ (\sum x_i )^2}{i} }{i-1} }
\end{equation}

\begin{equation}
\label{eqn:corr}
    corr(a, b) = \frac{ \sum(x_i \times y_i) - \frac{\sum x_i \times \sum y_i}{n} }{\sqrt{\sum x_i^2 - \frac{\sum x_i^2}{n}} \sqrt{\sum y_i^2 - \frac{\sum y_i^2}{n}}}
\end{equation}

As it can be seen from the equations, maintaining (i) number of observations, $n$; (ii) $\sum x_i$, sum of $i$ data points; (iii) $\sum x_i^2$, sum of squares of $i$ data points; and (iv) $\sum (x_i \times y_i)$, sum of cross product of $X$ and $Y$ are enough to recursively compute these statistics.


\subsubsection{Windowing}
Windowing is the process of selecting a subset of the observed instances that would be remembered to be used in the computation of statistics. Where the data set is finite and of limited size, all the instances can be remembered. For streams, this is not possible. Furthermore, computing statistics over all the instances of the past, in streaming environment would wrongly introduce information of classes that are not needed in the present. Thus, information of recent past is more important than the entire set. Windowing is categorized in two basic types: (i) sequence based windowing, and (ii) time-stamp based windowing.

In sequence based windowing, sequence is based on the number of observations seen; in time-stamp based approach, it is elapsed time. Landmark windowing and sliding windowing are two most used sequence based windowing system.

\paragraph{Landmark Windowing:} All observations after certain start point are remembered. Batched approaches can be thought of as examples of landmark windowing where every instance is remembered from the very first one. As new observations are seen, size of window increases. Landmark needs updating time to time to ensure recency of the statistics.

\paragraph{Sliding Windowing:} A fixed length window is moved through the observation set. As a new observation is seen, oldest observation is forgotten, i.e., when $j$-th instance is pushed into the window, $(j-w)$-th instance is forgotten, where $w$ is the size of the window. A limitation of sliding windowing is that it requires all elements within the window to be remembered, as it is needed to forget the oldest observation.

Often it is more useful to learn about most recent updates with fine granularity and older ones in a summarized fashion. With this motivation concept of tilted-time windowing was introduced~\cite{chen02:tiltedtime}.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=3.0in]{figs/naturaltime.pdf}
        \caption{Natural Tilted Time Window}
        \label{fig:bg:ntime}
    \end{center}
\end{figure}
\paragraph{Natural Tilted Time Windowing:} In natural tilted time windowing, units of time is distributed non-uniformly. Most recent time gets more units. For example, Figure~\ref{fig:bg:ntime} shows a natural tilted time windowing scheme, where for most recent hour 4 units stores quarterly updates, 24 units of time storing a day, and 31 units storing a months summary. That is, with 59 units of time information, this model stores about 32 days' information.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=4.0in]{figs/logtime.pdf}
        \caption{Logarithmic Tilted Time Window}
        \label{fig:bg:ltime}
    \end{center}
\end{figure}
\paragraph{Logarithmic Tilted Time Windowing:} Concept for logarithmic tilted time windowing is same as natural tilted time windowing. The only deference here is that time scale grows in logarithmic order. Figure~\ref{fig:bg:ltime} shows an example. 


\subsection{Change Detection}
\label{sec:bg:changedetection}
An assumption of most machine learning methods is that data is generated from a stationary distribution. As discussed in the previous section, this assumption does not hold for streaming scenario. Thus stream mining requires algorithms to facilitate drift detection methods. Differentiating between {\it noise} and {\it change} makes the problem challenging. The difference between a new distribution and noise is {\it persistence}, where new examples consistently follows new distribution rather than old. This section discusses several methods to detect and adapt learning algorithms in presence of concept drift.

\subsubsection{Detection}
Detection methods can generally be classified into two categories. First approach is to monitoring the evolution of various performance indicators as done in~\cite{klinkenberg98:changedetection, zeira04:changedetection}. Another approach is to maintaining two (or more) distributions varying the window length. Typically one window would summarize past history while the other would summarize most recent information~\cite{kifer04:condrift}.

Most methods follows the first approach. \cite{klinkenberg98:changedetection} monitors three performance indicators (accuracy, recall, and precision) over time, and uses their posterior comparison to a confident interval of standard sample errors for a moving average value for each indicator.

A classical algorithm for change detection is \textit{Cumulative Sum (CUSUM)}~\cite{page54:cusum}. CUSUM can detect that the mean of the input data is significantly different than zero. The test is as follows:
\[
    g_0 = 0 
\]\[
    g_t = max (0, g_{t-1} + (r_t - v))
\]
if $g_t > \lambda$ CUSUM triggers an alarm and set $g_t = 0$. This detects the changes in the positive direction, to detect the negative change $min$ is used instead of $max$. CUSUM does not require any memory. Its accuracy depends on the choice of $v$ and $\lambda$. Low $v$ results in faster detection with more false alarms.

For latter category, \cite{kifer04:condrift} uses Cherfnoff bound (Theorem~\ref{thm:chernoff}) and examines examples drawn from two probability distribution and decides whether these distributions are different.

\paragraph{ADWIN Algorithm:} ADWIN (ADaptive sliding WINdow) is another change detection algorithm of latter type. ADWIN keeps a variable length window of recent items. It ensures the property {\it there has been no change in the average value inside the window} for maximally statistically consistent length. The core idea of ADWIN is that whenever two {\it large enough} sub-windows of $W$ exhibit {\it distinct enough} averages, it is assumed that their corresponding expected values are different, and the older portion of the window is dropped. Essentially, this means that when the difference of means of the two windows is greater than a certain threshold $\epsilon_{cut}$, the older portion should be dropped. Equation to compute $\epsilon_{cut}$ is as follows:
\[
    m = \frac{2}{1/|W_0| + 1/|W_1|}
\]\[
    \epsilon_{cut} = \sqrt{\frac{1}{2m} \ln \frac{4 |W|}{\delta}}
\]
where $\delta \in (0, 1)$ is confidence value, an input.

\begin{algorithm}[htbp]
    \DontPrintSemicolon
\label{alg:adwin}
\caption{ADWIN Algorithm}

    \KwData{Data Stream}
    \KwResult{Window with most recent concept}
    \Begin{
        Initialize window $W$ \\
        \ForEach{$t > 0$} {
            $W \leftarrow W \cup \{x_t\}$ \tcp*[f]{add $x_t$ to the head of $W$ }\\
            \Repeat{$|\mu_{W_0} - \mu_{W_1}| < \epsilon_{cut}$ holds for every split of $W$} {
                Drop element from the tail of W\\
            }
            $W = W_0 . W_1$\\
        }
        Output $\mu_W$\\
    }
\end{algorithm}

ADWIN does not maintain the window explicitly, but compresses it using a variant of the exponential histogram technique. This means that it keeps a window of length $w$ using only $O(\lg w)$ memory and $O(\lg w)$ processing time per item.

\subsubsection{Adaptation to Change}
To improve accuracy of the decision model under concept drifting environment, decision model needs adaptation to the changes. There are two types of approaches based on when to adapt: 
\begin{itemize}    
    \item Blind or Periodic Methods: Models are updated on regular interval whether any change has actually occurred or not. 
    \item Informed Methods: Models are only updated when there are sufficient reasons to believe that changes in the concept have occurred.
\end{itemize}
It could be a good idea to use periodic methods where duration of seasonality is known beforehand. Otherwise, chosen duration could be too large or too small to response to the changes. In such cases, an informed decision is more desired. However, informed methods requires more resources (than blind methods) to be able to make such decision.

\subsection{Na\"ive Bayes Adaptation}
Naive Bayes algorithm is essentially a stream classification algorithm. One of the advantages of this classifier in the context of data stream is its low complexity for deployment. It only depends on the number of explanatory variables. Its memory consumption is also low since it requires only one conditional probability density estimation per variable.

\begin{algorithm}[htbp]
    \caption{VFDT: The Hoeffding Tree Algorithm}
    \label{alg:vfdt}
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input} \SetKwInOut{Output}{Output} 
    
    \Input{$S$: Stream of examples \\
        $X$: Set of nominal attributes \\
        $Y$: Set of class labels $Y = \{y_1, y_2, \dots, y_k\}$ \\
        $G(.)$: Split evaluation function \\
        $N_{min}$: Minimum number of examples \\
        $\delta$: is one minus the desired probability \\
        $\tau$: Constant to resolve ties
    } 
    \Output{$HT$: is a decision tree}
    
    \Begin{
        Let $HT \leftarrow$ Empty Leaf (Root)
        \ForEach{$example(x, y_k) \in S$} {
            Traverse the tree $HT$ from root till a leaf $l$ \\
            
            \eIf (\tcp*[f]{Missing class label}) {$y_k == ?$ } {
                Classify with majority class in the leaf $l$
            } {
                Update sufficient statistics \\
                \If{$ Number\;of\;examples\;in\;l > N_{min}$ }{
                    Compute $G_l(X_i)$ for all attributes \\
                    Let $X_a$ be the attribute with highest $G_l$ \\
                    Let $X_b$ be the attribute with second highest $G_l$ \\
                    Compute $\epsilon = \sqrt{\frac{R^2 \ln(2/\delta)}{2n}}$  \tcp*[f]{Hoeffding bound} \\
                    
                    \If{$G(X_a) - G(X_b) > \epsilon\; || \;\epsilon < \tau$} {
                        Replace $l$ with a splitting test based on attribute $X_a$ \\
                        Add a new empty leaf for each branch of the split \\
                    }
                }
            }
        }
        Return $HT$
    }

\end{algorithm}

\subsection{Very Fast Decision Tree}
\label{sec:bg:vfdt}
Very Fast Decision Tree (VFDT), also known as Hoeffding Tree (HT), is the stream adaptation of decision tree generating from a stationary distribution. It is an anytime-ready algorithm and uses Hoeffding bounds to ensure the performance in terms of accuracy is asymptotically nearly identical to that of conventional tree algorithms. VFDT runs on constant time and memory per examples and can serve thousands of examples on a typical consumer system.

VFDT constructs the tree by recursively replacing leaves with decision nodes. Each leave stores sufficient statistics that are used to evaluate the merit of split-test and to decide the target class. For incoming instances, the tree is traversed from the root to a leaf (based on the incoming instance's values) and the statistics are updated accordingly. If a unanimous decision cannot be reached based on observed instances at that particular leaf node, the node is tested to check the sufficiency for a split. If there is enough statistical support in favor of split, the node is splitted on the best attribute and stats are passed to the descendants (new leaves). Number of descendants of this new decision node is equal to the number of possible values of the chosen attribute. Thus, the tree is not necessarily a binary tree.

Deciding on whether to split a node or not is a unique contribution of VFDT. VFDT solves this difficult problem of deciding exactly how many examples are required to be observed by a leaf node before splitting by using Hoeffding bound (Theorem~\ref{thm:hoeffding}). Consider $G(.)$ be the heuristic measure of the attributes. This measure could be information gain as of C4.5 or Gini index of CART. For information gain range $R$ in Hoeffding bound is $\lg (\#classes)$. Goal is to find $n$ such that the attribute chosen for split after observing $n$ instances, would, with high probability, be the same as it would be chosen after observing infinite instances. Assume that $X_a$ is the attribute with the best $G(.)$, and $X_b$ is the second best attribute after observing $n$ instances. Then $\Delta G = G(X_a) - G(X_b)$ be the difference between their observed heuristics. From Hoeffding bound, we know if $\Delta G < \epsilon$, then with probability $1 - \delta$ $X_a$ would be the attribute with highest value in the evaluation function in the universe. Otherwise, if $\Delta G < \epsilon$, then the sample size is not enough to make a stable split decision. As the assumption is that underlying generating distribution is stationary, thus as the sample size increases, $\epsilon$ decreases and heuristic value for most informative attribute goes up.

To fasten up the process, VFDT uses an extra parameter $N_{min}$ to reduce the number of $G(.)$ computation. Computing $G(.)$ when there are too few instances is run-time inefficient. Thus a user parameter $N_{min}$ is used to indicate minimum number of instances needed to be observed before the evaluation starts.

When multiple attributes continuously perform similar in heuristic evaluation after observing a large number of examples, $\Delta G$ might never be greater than $\epsilon$. To break such tied cases, another user parameter $tau$ is used, and when $\epsilon$ falls below $\tau$ (i.e. $\Delta G < \epsilon <\tau$), algorithm splits on the best attribute. Algorithm~\ref{alg:vfdt} summarizes the pseudo-code of VFDT.

There are some property of VFDT that are different than C4.5. In contrast to the C4.5 algorithm number of examples that support a decision increases in VFDT. Typically VFDT also results a low variance model than that of C4.5. However, there is no mechanism avoid over-fitting in VFDT as there is no room for pruning.

\begin{algorithm}[htbp]
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input} \SetKwInOut{Output}{Output} 
    \caption{CVFDT: Concept-adapting VFDT}
    \label{alg:cvfdt}
    
    \Input{$S$: Stream of examples \\
        $X$: Set of nominal attributes \\
        $Y$: Set of class labels $Y = \{y_1, y_2, \dots, y_k\}$ \\
        $G(.)$: Split evaluation function \\
        %$N_{min}$: Minimum number of examples \\
        $\delta$: is one minus the desired probability \\
        $\tau$: Constant to resolve ties \\
        $w$: Size of the window \\
        $n_{min}$: Number of examples between checks for growth \\
        $f$: Number of examples between checks for drift
    } 
    \Output{$HT$: is a decision tree}
    
    \Begin{
        Let $HT \leftarrow$ Empty Leaf (Root) $l_0$ \\
        Let $Alt(l_0) \leftarrow \emptyset$ \tcp*[f]{Alternate trees for $l_0$} \\
        %Let $G(X_\emptyset)$ be the $G(.)$ obtained by predicting most frequent class \\
        %Let $X_1 = X \cup \{X_\emptyset\}$ \\
        
        \ForEach{class $y_k$} {
            \ForEach{$x_{ij} \in X_i \in X$} {
                Set $n_{ijk} = 0$
            }
        }
        \ForEach{$example(x, y) \in S$} {
            Traverse the tree $HT$ including $Alt(\{n: (x,y)\;passes\;through\;n\;in\;HT\})$ trees root till a set of leaves $L$ \\
            
            Set $id = \max(\{L.id\})$ \\
            Add $((x, y), id)$ to the beginning of $W$ \\
            \If{$|W| > w$} {
                Let $((x_w, y_w), id_w)$ be the last element in $W$ \\
                FORGET($HT, n, (x_w, y_w), id_w)$) \label{algln:cvfdt:forget} \\
                $W = W - ((x_w, y_w), id_w)$
            }
            GROW($HT, n, G, (x,y)$ \\
            \If{examples seen since last checking $> f$}{
                VALIDATE\_SPLIT($n$) \label{algln:cvfdt:validatesplit} 
            }
        }
        Return $HT$
    }
\end{algorithm}
\subsection{Concept-adapting Very Fast Decision Tree}
Concept-adapting Very Fast Decision Tree (CVFDT) is the extension of CFDT that adds the ability to adapt the model by detecting the changes in the underlying distribution that generates the examples. CVFDT does not require the model to be recomputed, instead it updates the sufficient statistics stored in each node that the new instances affects by maintaining a sliding window. It increases the counter of new instances and decreases count of oldest examples which now need to be forgotten. If the underlying distribution is stationary, this would not have any effect. But in case of a concept drifting distribution, some decision nodes that previously had passed the Hoeffding bound test will no longer pass, rather an alternate attribute would now have higher or similar gain. CVFDT keeps stats to detect such situation and starts maintaining an alternate subtree with the new best attribute as its root. When this alternate subtree becomes more accurate on new data then the old subtree is replaced by the new one. Pseudo-code of CVFDT has be shown in Algorithm~\ref{alg:cvfdt}. The sub-routine $GROW$ is essentially the Hoeffding Tree algorithm where instead of only keeping stats in the leave, every node keeps track of the instances it has seen. Two other sub-routines $FORGET$ and $VALIDATE\_SPLIT$ have been shown separately.

\begin{algorithm}[htbp]
    \ContinuedFloat
    \DontPrintSemicolon
    \LinesNumberedHidden
    \setcounter{algoline}{0}
    \newcommand\Numberline{\refstepcounter{algoline}\nlset{\thealgoline}}
    \SetNlSty{textbf}{\ref*{algln:cvfdt:forget}.}{}
    
    \SetKwInOut{Input}{Input} \SetKwInOut{Output}{Output} 
    \SetKwInOut{TitleOfAlgo}{Algorithm}
    \TitleOfAlgo {FORGET($HT, n, (x_w, y_w), id_w$)}
    \label{alg:cvfdt:forget}
    
    \Begin{
        \Numberline Sort $(x_w, y_w)$ through $HT$ while it traverses leaves with $id \le id_w$ \\
        \Numberline Let $P$ is the set of sorted nodes \\
        
        \Numberline \ForEach{node $l \in P$} {
        \Numberline     \ForEach{$x_{ij} \in x : X_i \in X_l$} {
        \Numberline         Decrement $n_{ijk}(l)$
            }
        \Numberline     \ForEach{tree $t_{alt} \in Alt(l)$} {
        \Numberline         FORGET($t_{alt}, n, (x_w, y_w), id_w$)
            }
        }
    }
\end{algorithm}

$FORGET$ function is used to remove the effect of instances of older concepts. However, this is not straight-forward as HTs go through changes after the instance to be forgotten was initially added. Thus, CVFDT uses monotonically increasing ids for the nodes. When a new instance is added to the window ($W$), the maximum $Id$ of the leaves it reaches in $HT$ and all the alternate trees is recorded with it. To remove the effect of a old instance, counts are decremented at every node the example reaches in $HT$ whose $Id$ is less than the stored $Id$.

\begin{algorithm}[htbp]
    \ContinuedFloat
    \DontPrintSemicolon
    \LinesNumberedHidden
    \setcounter{algoline}{0}
    \newcommand\Numberline{\refstepcounter{algoline}\nlset{\thealgoline}}
    \SetNlSty{textbf}{\ref*{algln:cvfdt:validatesplit}.}{}
    
    \SetKwInOut{Input}{Input} \SetKwInOut{Output}{Output} 
    \SetKwInOut{TitleOfAlgo}{Algorithm}
    \TitleOfAlgo {VALIDATE\_SPLIT($HT, n, (x_w, y_w), id_w$)}
    \label{alg:cvfdt:validatesplit}
    
    
    \Begin{
        \Numberline Let $HT \leftarrow$ Empty Leaf (Root) $l_0$ \\
        \Numberline Let $Alt(l_0) \leftarrow \emptyset$ \tcp*[f]{Alternate trees for $l_0$} \\
        
        \Numberline \ForEach{internal node $l$ in $HT$} {
        \Numberline     \ForEach{$t_{alt} \in Alt(l)$} {
        \Numberline         VALIDATE\_SPLIT($t_{alt}, n, (x_w, y_w), id_w$)
                    }
        \Numberline     \ForEach{$x_{ij} \in X_i \in X$} {
        \Numberline         Set $n_{ijk} = 0$
                        }
                    }
        \Numberline Let $X_a$ be the current split attribute \\
        \Numberline Compute $\Delta(G) = G_l(X_n) - G_l(X_b)$  \tcp*[f]{$X_n, X_b$ two current highest 		attributes} \\
        
        \Numberline \If{$\Delta(G) \ge 0 \&\& X_n \not\in \{roots of Alt(l)\}$ } {
        \Numberline     Compute $\epsilon = \sqrt{\frac{R^2 \ln(2/\delta)}{2n}}$  \tcp*[f]{Hoeffding bound} \\
        \Numberline     \If{$\Delta(G) > \epsilon\; || \;\epsilon < \tau \&\& \Delta(G) \ge \tau/2$} {
        \Numberline         Let $l_{new}$ be an internal node that splits on $X_n$ \\
        \Numberline         $Alt(l) = Alt(l) + l_{new}$ \\
        \Numberline         \ForEach{branch of the split} {
        \Numberline             Add a new leaf $l_m$ to $l_new$ \\
        \Numberline             $X_m = X - {X_n}$ \\
        \Numberline             $Alt(l_m) = {}$ \\
                    
        \Numberline             Compute $G_m(X_\emptyset)$ using most frequent class at $l_m$ \\
        \Numberline             \ForEach{node $l \in P$} {
        \Numberline                 \ForEach{$x_{ij} \in x : X_i \in X_l$} {
        \Numberline                     Decrement $n_{ijk}(l)$
                        }
                    }
                }
            }
        }
    }
\end{algorithm}

Lastly, there is $VALIDATE\_SPLIT$ sub-routine that periodically checks the internal nodes of $HT$ where current split attribute would no longer have the highest $G(.)$ or $\Delta G$ would be less than $\epsilon$ or $\Delta G$ would be less than $\tau/2$. This is similar condition as to the original one with more restrictive tie condition. The added restriction ensures less alternate tree creation.

Now that related concept of basic learning algorithms and stream mining are discussed, in the next section, the concepts of ensemble learning are presented.

\section{Ensemble Learning}
Ensemble learning, due to its intrinsic merits, focuses to get the best out of a collection of base learners. Ensemble learning can be thought of as a divide-and-conquer approach. However, it may not necessarily divide the tasks rather might do more tasks. The motivation behind was presented in the previous chapter. In short, ensemble methods are interesting because of following possibilities:
\begin{itemize}
    \item Avoiding worst classifier by averaging several classifier.
    \item Fusing multiple classifier to improve performance of the best classifier.
    \item Arranging the better classifiers (might include the best one) in a way to outperform the best one.
    \item Dividing streams into chunks, learn separate models from each and then combine their results (divide and conquer).
\end{itemize}

A number of methods have been developed in past couple of decades focusing on these motivations. They employ different approaches to improve the final classifier. Following is a summary of different approaches of generating ensembles of classifiers.
\begin{itemize}
    \item Creating multiple training sets by resampling the original set and feeding those into different learners.
    \item Using different combination of features to learn multiple classifiers.
    \item Manipulating class labels. For example, transforming the classes into binary classification problem by partitioning the class labels into disjoint subsets.
    \item Manipulating the learning algorithms such that they result different outcomes for the same training set. For example, introducing certain randomness into tree growing algorithm.
\end{itemize}

In this section, we present these basic concepts of building an ensemble of a collection of classifier. We start with primitive methods such as bagging and boosting. However, we are particularly interested in more sophisticated methods based on decision tree learning methods of stream data such as Adaptive Size Hoeffding Tree (ASHT) and ADaptive WINdow bagging (ADWIN bagging).

\subsection{Bagging}
\label{sec:bg:bagging}
Bagging, also known as bootstrap aggregating, is a meta algorithm that improves accuracy and reduces variance and chance of over-fitting. Bagging is typically used with tree based learners. Given a training set $D$ of size $n$, bagging generates $m$ new training sets $D_i$ of size $n_{new}$ where $i = \{1, 2, \dots, m\}$ by sampling with replacement. If $n = n_{new}$ approximately two-thirds of the instances of $D_i$ is expected to be unique examples of $D$ while rest being duplicates. If $K$ is the number of examples belonging from the original training set then $P(K=k) = \binom{n}{k} \left( \frac{1}{n}\right)^k \left(1- \frac{1}{n}\right)^{n-k}$. These $m$ sets are then used to learn $m$ classifier. Final decision is given by a majority voting scheme over the decision of these $m$ classifiers. Figure~\ref{fig:bg:bagging} shows an illustrative example of bagging method.
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=10.0cm]{figs/bagging.jpg}
        \caption{Bagging or bootstrap aggregation}
        \label{fig:bg:bagging}
    \end{center}
\end{figure}

For online bagging approach $n \rightarrow \infty$, and the Binomial distribution of $P(K=k)$ tends to a Poisson distribution with $P(K=k) = \exp(-1)/k!$. Using this justification, online bagging chooses a $k ~ Poisson(1)$ for each incoming training examples $(x_i, y_i)$, and updates the base learning learning algorithms $k$ times. The classification is done as the same way of batched approach, by unweighted voting of $m$ base classifiers. For similar distribution of training examples, online bagging produces similar approximated base models. If (i) used based learner converges to the same same classifier with increasing training examples, and (ii) base learner produces same classifier given a fixed training set for both batched and online bagging methods; then online bagging will converge to the classifier obtained though batched learning methods.


\subsection{Boosting}
Boosting is another supervised learning algorithm to iteratively improve learning hypothesis. The motivation behind the boosting approach is that a set of weak classifier could create a single strong learner. It forces a weak classifier to update or generate new rules that make less mistakes on previously misclassified records. Initially all records are assigned same weights. At the end of a boosting round, weights might change due to the errors in classification. Weights of the instances are increased or decreased if they are classified wrongly or correctly, respectively. Thus successive classifiers depend upon their predecessors. 
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=10.0cm]{figs/boosting.pdf}
        \caption{Boosting method example}
        \label{fig:bg:bagging}
    \end{center}
\end{figure}

Mathematically, let $H= \{h_1, h_2, \dots, h_n\}$ be $n$ weak hypotheses. The combined hypothesis $H(.)$ is a weighted majority vote of the $n$ weak hypotheses where each hypothesis $h_i$ has a weight of $\alpha_i$ for $i = \{1, 2, \dots, n\}$:
\begin{equation}
\label{eqn:adaboostHypo}
    H(.) = sign \left(\sum_{i=1}^n \alpha_i h_i(x) \right).
\end{equation}

One of the earliest boosting approach was proposed in~\cite{schapire90:whyens}. It calls weak learners three times on three modified distributions and achieves slight improvement in accuracy. Later,~\cite{freund97:boosting} proposed AdaBoost, an adaptive boosting method, that works with a principle of minimizing upper bound of empirical error. Let, $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$ are given examples where $x_i \in X, y_i \in Y = [-1, +1]$. Let, $D_t (i)$ be the weight of $i$-th example at $t$-th round. Then, AdaBoost initializes $D_1(i) =1/n$. Iterative steps are as follows:

For each iteration $t= 1, 2, \dots, T$:
\begin{itemize}
    \item Train weak learner using distribution $D_t$
    \item Get weak hypothesis $h_t : X \rightarrow \{-1, +1\}$ with error
    \[
        \epsilon_t = Pr_{i ~ D_t} [h_t(x_i) \ne y_i]
    \]
    \item Choose $\alpha_t = \frac{1}{2} \ln \left( \frac{1 - \epsilon_t}{\epsilon_t} \right)$
    \item Update $D_{t+1} (i) = \frac{D_t(i) \exp(-\alpha y_i h_t(x_i))}{Z_t}$ where $Z_t$ is a normalization factor
\end{itemize}
Finally, the output is calculated with the Equation~\ref{eqn:adaboostHypo} given above.

An online variant of AdaBoost uses a similar approach as online bagging method, simulating sampling with replacement using Poisson distribution. However, the Poisson parameter $\lambda$ (Equation~\ref{eqn:poisson}) associated with an example is increased if that particular example is misclassified. In case of correct classification $\lambda$ is decreased. Similar to the AdaBoost, online boosting approach assigns total weights equally to the correctly and misclassified instances, i.e. half of the total weights each. Unlike AdaBoost, in online boosting weights are updated based on only the examples seen thus far rather than the complete training set. This is intuitively problematic as initial hypotheses are build on too few examples. Even with this limitation, online boosting shows good performance. Online boosting with naive Bayes base learner converges to the model achieved with AdaBoost as the number of training instances tends to infinity.

\subsection{Adaptive-Size Hoeffding Tree (ASHT) Bagging}
In previous section (Section~\ref{sec:bg:vfdt}), we have introduced Very Fast Decision Tree (VFDT) or Hoeffding Tree (HT). Hoeffding Tree is inspired by the fact that a small sample size could be enough to effectively choose an optimal splitting attribute. An upper bound of the error introduced because of such generalization is given using Hoeffding bound.

The Adaptive-Size Hoeffding Tree (ASHT) is, as the name suggests, extended from Hoeffding tree with deletion of nodes or reseting of the tree capabilities. Following are two significant differences of adaptive-size Hoeffding tree with Hoeffding tree: 
\begin{itemize}
    \item Maximum number of split nodes or the size of the tree is bounded in ASHT. There is no such limit on HT. HT grows indefinitely as the data and new concepts are introduced.
    \item When number of split nodes exceeds the maximum value (essentially after a new split), some nodes are deleted to retain the tree property (max size).
\end{itemize}
There are two different choices for the deletion of nodes for the second point. First option is to delete the oldest node. In Hoeffding tree, root is always the oldest node. All children of root except for the nodes that were further splitted are also deleted in this case. The new root would be the root of the child not being deleted. Another and more crude approach would be to delete the complete altogether i.e. reseting the tree. 

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=10.0cm]{figs/asht.jpg}
        \caption{Adaptive Size Hoeffding Tree concept}
        \label{fig:bg:asht}
    \end{center}
\end{figure}

Based on this modified Hoeffding tree, a bagging method is developed in~\cite{bifet09:asht}. The intuition of the methods is as follows: smaller trees adapts to changes faster than the larger trees. However, larger trees perform better where data has only small changes or drifts because of the fact the it is built with more data. A tree of size $s$ would expected to be reset twice as often as a tree with size $2s$. Using an ensemble of different sizes would thus give a set of trees with different reset speeds. Smaller trees would be updated for smalls changes in the streams, while larger ones will maintain a history for longer period. Polling over this set of classifier would thus expected to result in a classifier with finer granularity. It is to be noted that reset will occur even for stationary data, however, this should not have any negative influence on the ensemble's predictive performance for stationary cases.

The proposed bagging method in~\cite{bifet09:asht} uses $n$ adaptive-size Hoeffding trees. The $n$-th ASHT has twice the maximum size of $(n-1)$-th tree. Size of the smallest tree is 2. For the polling, each tree is associate with a weight, which is selected to be the inverse of squared error.

\subsection{ADWIN Bagging}
ADWIN Bagging combines three different powerful concepts together: (i) bagging (Section~\ref{sec:bg:bagging}), (ii) adaptive window change detection (Section~\ref{sec:bg:changedetection}), and (iii) Hoeffding tree (Section~\ref{sec:bg:vfdt}).

Bagging using ADWIN is implemented as ADWIN Bagging where the Bagging method is the online bagging method of Oza and Rusell~\cite{oza01:obagboost} with the addition of the ADWIN algorithm as a change detector. Hoeffding Trees are used as base classifier. When a change is detected, the worst classifier of the ensemble of classifiers is removed and a new classifier is added to the ensemble. 

\bibliographystyle{apalike}
\bibliography{ref}

\end{document}
