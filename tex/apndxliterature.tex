\chapter{Extended Related Works}
\label{appndx:erw}
\section*{Stream Learning}
In a recent approach, \cite{rutkowski13:vfdt} argued that using McDiarmid’s bound instead of the Hoeffding bound in VFDT is more appropriate for ensuring the approximation bound i.e. the split decisions made after seeing a certain number of instances will, with high probability, remain the same for a infinite number of instances. The authors also presented McDiarmid’s bound for the  information gain of the ID3 algorithm, and the Gini index of the CART algorithm. It is to be noted that the Hoeffding bound is a special case of the McDiarmid’s bound. In another paper, \cite{matuszyk:vfdt} showed that the usage of Hoeffding bound is mathematically flawed as (i) the Hoeffding inequality only applies to an arithmetic average, which information gain and Gini index are not, (ii) the values obtained in sliding window methods are not independent, while the Hoeffding inequality only applies to independent random variables. A revised bound showed that the decision bound should be twice of the one given by the Hoeffding bound, otherwise,  the error-likelihood should be updated accordingly.


A different decision tree based approach based on the so-called \textit{Peano Count Tree (P-tree)} has been developed by Ding et al. for spatial data streams~\cite{ding02:peanocount}. The Peano Count Tree is a spatial data structure that facilitates a lossless compressed representation of spatial data. This structure is used for fast calculation of the information gain for branching in decision trees.

Aggarwal et al. employs a slightly different idea in handling time-evolving data in their on demand classification approach of data streams ~\cite{aggarwal04:ondemand}. They used a modified \textit{micro-cluster concept} introduced in~\cite{aggarwal03:clustream}. Micro-clusters are created from the training data stream only. Each micro-cluster corresponds to a set of points from the training data belonging to the same class. To maintain statistics over different time horizons and to avoid the storage of unnecessary data points a geometric time frame is used. In the classification task, the \textit{$k$-nearest neighbor} based approach is taken, where micro-clusters are treated as node weighted by their instance counts. 

In~\cite{ganti02:gemm:focus} two algorithms named GEMM and FOCUS have been introduced for streams under block evolution. GEMM is used for model maintenance and FOCUS is used for change detection between two data stream models. These algorithms have been tested using decision trees and frequent item set models. FOCUS uses bootstrapping methods to compute the distribution of deviation values when data characteristics remain the same. This distribution is then used to check whether the observed deviation value indicates a significant deviation. In another approach to handle concept drift, Last~\cite{last02:olin} proposed an online classification system OLIN that would dynamically adjust the size of the training window and the number of new examples between model re-constructions to the current rate of concept drift. OLIN uses constant resources to produce models, and achieves nearly the same accuracy as the ones that would be produced by periodically re-constructing the model from all accumulated instances.


In a different approach to handle concept drift Castillo et al.~\cite{gama03:drift} used Shewhart P-Charts in an online framework based on the idea of \textit{Statistical Quality Control}. Two alternatives of P-charts were used to monitor the stability of one or more quality characteristics in a drifting stream. The two alternatives only differed in the methods they use to estimate the target value to set the center. The group later introduced another drift detection scheme that monitors the probability distribution of examples and maintains an online error rate to detect any concept drift~\cite{gama04:drift}. When the distribution changes, the error rate will increase. For a stationary concept, the error rate should always gradually decrease. A new concept is said to be started if the error rate exceeds some predefined warning or threshold level. This approach has been used in the \textit{Ultra Fast Forest Tree (UFFT)}~\cite{gama04:ft, gama05:ft} stream classification method. UFFT maintains naive Bayes statistics for every node. If at any node the error rate starts increasing, the node is pruned for drifting concept. UFFT uses a similar approach and the Hoeffding bound to control the growth of the tree. For each pair of classes a tree is maintained, hence it is called forest-of-trees.

Later, Aggarwal proposed another concept drift technique based on velocity density estimation \cite{aggarwal03:condrift}. Velocity density estimation is a technique to understand, visualize, and determine trends in the evolving data. The work presented a scheme to use velocity density estimation to create temporal velocity profiles and spatial velocity profiles at periodic instants in time. These profiles are then used to predict dissolution, coagulation, and shift in data. The proposed method could detect changes in trends in a single scan with a linear order of number of data points. Additionally, a batch processing technique to identify combinations of dimensions which results in the greatest amount of global evolution is also introduced. In~\cite{kifer04:condrift} authors tried to formally define and quantify the change so that existing algorithms can precisely specify when and how the underlying distribution has changed. They employed a two fixed-length window model, where a current one is updated every time a new example arrives and a reference one is only updated when a change has been detected. To compare the distributions of the windows, $L1$ distance has been used. Another method to compare two distributions has been presented in~\cite{dasu05:condrift} where authors used the Kullback-Leibler (KL) distance to compare two distributions. The KL distance is known to be related to the optimal error in determining the similarity of two distributions. In this non-parametric method no assumptions on the underlying distributions is required. 


\section*{Ensemble Learning}

Online bagging and boosting methods do not particularly give attention to the concept drifting nature in the data. \textit{Accuracy Weighted Ensemble (AWE)}~\cite{wang03:accuweighted} is one of the earliest works on concept-drifting stream data. AWE assumes that the stream is delivered in chunks of defined size. With each incoming chunk, AWE updates its $k$ classifiers. Each classifier has an associated weight which is inversely proportional to the expected error of the respective classifier. To estimate this error, it is assumed that the distribution of test set is the closest to the most recent chunks. Concept drift is adapted by effectively manipulating the number and the magnitude of the weights that are changing. An extension of AWE has been proposed in~\cite{brzezinski11:accuupdated}, namely \textit{Accuracy Updated Ensemble (AUE)}. AUE takes the weighting motivation from AWE, but improves the limitation of AWE. In AWE each classifier learns from the incoming chunks in a ``batched'' fashion. AUE employs an online scheme instead. AUE also adapts the weighting function to reduce the adverse effect in AWE of sudden drift in the data. The AWE weighting function is prone to suffer by a rapid change in the stream and most or even all classifiers assuming they are ``risky''. This limitation has been addressed in AUE. Result shows that AUE performs marginally better than AWE, however, also it requires slightly longer time and larger space.

%ens in stream

Another group of methods known as \textit{random forests} uses bagging of decision trees. The concept of random forest is introduced by Breiman~\cite{breiman99:randomforest}. Random forest works in a similar manner as bootstrap aggregation. However, for each split in the tree it only selects a subset of the features to be considered as splitting criterion. This \textit{feature bagging} approach helps to avoid very strong predictors to get selected over and over. One method mentioned before, \textit{Ultra Fast Forest Tree (UFFT)}~\cite{gama04:ft, gama05:ft} uses concepts of tree bagging, however, works with all features the whole time. UFFT maintains statistical information on each node to detect drift and grows the tree in a similar  approach to VFDT. Using the statistical information stored in the nodes it detects concept drift with a na\"ive Bayes error-rate. Abdulsalam et al. proposed \textit{Dynamic Streaming Random Forests (DSRF)} focusing on lowering the number of examples required to build up new model in a drifting environment~\cite{salam08:dsrf, salam11:dsrf}. Here, Shannon entropy~\cite{shannon01:entropy} is used to detect concept drift. All these methods are empirically proven to be effective for a generalized scenario and chosen data sets. 


\clearpage
