\chapter{Experimental Evaluation}
\label{chp:exp}
In this chapter we present the empirical analysis of the discussed algorithms. We analyze the performance and properties of these algorithms varying wide range of parameters. We compare our approach described in Chapter~\ref{chp:algo} with the current state-of-the-art methods presented in Chapter~\ref{chp:background}. Each of the analysis is performed simultaneously- both for data streams generated by random RBF generator and our newer approach that generate speed varied RBF streams (Chapter~\ref{chp:dataset}). A 64-bit Windows 7 machine with Intel Core i7 dual-core processor clocking up to 2.4 GHz and 8 GB of RAM is used to run the experiments. Primary implementation is derived from Java based MOA framework~\cite{bifet:moa10}. 


\section{Evaluation Process}
There are two different approaches in evaluating algorithms in streaming environment: (i) holdout evaluation, and (ii) prequential evaluation. In holdout evaluation, two separate train and test sets are used. Once training is done, typically using one-single pass, using the training set, test set is used to evaluate the performance. This is ideal where there are large amount of data, where it is expected that with splited test set, chance of over-fitting would be very low. In prequential evaluation, each example used to test the model in hand, and then the instance is used to train (update) the model. This method, thus, continuously changes the observed performance metrics and the model. Prequential model is, particularly, a necessity when the classifier has to ensure that the model is update for the newest instances. Well known cross validation approach is trivially not applicable in streaming environment. In our setting, prequential evaluation is most appropriate. Binary class datasets of 1,000,000 instances in a 10 dimensional hyperspace are generated using random RBF generator and variable speed RBF generator for the evaluation. 

\subsection{Performance Metrics}
A number of metrics have been used for the evaluation criteria. Accuracy, processing time, memory usage, and kappa statistics are primarily used for the evaluation along with various tree structure related parameters such as tree depth, tree size, number of decision nodes, etc. Moreover, we observed algorithm specific properties e.g. number of tree reset, number of pruned trees, number of maintained alternate trees, number of times of partial tree replacement, etc. These algorithm specific parameters are not always directly comparable. Parameters are comparable among HT, adaptive HT, and ASHT. Bagging and boosting approaches have a different setting. Thus, we used weighted averages for these methods, when calculating number of reset or pruning type of stats. For example, in bagging methods, smaller trees reset more often, thus while calculating number of resets, a lower relative weight is given to the smaller trees. In our experiments, weights are proportional to the maximum size limit, or equal where there is no size restriction.


\section{Study case: Census Income Dataset}
To begin with, we check the performance of the stream algorithms in comparison to the classical batched data mining algorithms. For the evaluation, census income dataset~\cite{ron:adultds} is been used. This dataset contains 48,842 instances of 14 attributes (6 continuous, 8 categorical) mapping to a binary ``income'' class (details in Appendix~\ref{appndx:ci}). Most batched learning methods such as C4.5, SVM, MLP achieve an accuracy around 82-83\% with 10 fold cross validation for the income class prediction task. As it can be seen from Figure~\ref{fig:exp:ci}, stream leaning methods also achieve an accuracy within 1-2\% percent less than the batched learners. Measured F1 value also presents similar scores. Though margins are insignificant, however, ASHT and bagging with ASHT seem to perform relatively worse than other algorithms. It is evidently because of the resetting of the trees. After each reset, few instances will suffer because the new trees are still very small. On the other hand, with our proposed modifications, bagging with SRHT overcomes this problem, and performs nearly same as C4.5.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-3mm}\resizebox{75mm}{!}{\includegraphics{res/{0-ci-algo-accu}.pdf}} &
            
            \hspace{-5mm}\resizebox{75mm}{!}{\includegraphics{res/{0-ci-algo-fm}.pdf}} \\
            \scriptsize{(a)\vspace{2mm}} &
            \scriptsize{(b)}    
        \end{tabular}
        \caption{Comparison of classical and stream learning algorithms with census income dataset (a) accuracy and (b) F1-measure }
        \label{fig:exp:ci}
    \end{center}
\end{figure}


In terms of tree structure, final classifier tree from stream learner are simpler than the C4.5 tree. While C4.5 has many decision rules with over 10 decision nodes, Hoeffding tree based approaches are found to be performing similar with only 4 decision nodes. It is to be mentioned here that with different ordering of the input data, a higher decision node count is a possibility.

Lastly, we simulated a concept evolving scenario by controlled sampling of data from different clusters within the census income data. Five clusters were found using batched learning methods. While it had no significant effect on the performance of HT, however, methods that uses Adwin is found to be get reset often depending on the sampling sequence. When each clusters were used sequentially, they reset all 4 times when concept were changed.


\section{Parameter Analysis}
The evaluated algorithms depends on a number of parameters. Grace period, tie threshold, and binary tree of not are the most important parameters for HT, adaptive HT, and ASHT. Similarly, there are parameters that controls the properties of generated. For bagging and boosting methods ensemble size is important. Additionally for ASHT and SRHT we have first classifier size, choice of reset or pruning are also important. Similarly, there are parameters on which the data generation processes (Chapter~\ref{chp:dataset}) rely on. In this section, we analyze the effect of each of these parameters starting with parameters for data generation. For the discussion here, we used most relevant metrics related to the each parameters. A comprehensive list of plots are given in the Appendix~\ref{appndx:plots} detailing all related metrics.

\subsection{Effect of Drift Coefficient}
Previously, in Figure~\ref{fig:ds:rbfdrift} it was shown that how generated data is distributed on a 2D space. Compared to Figure~\ref{fig:ds:rbfdrift}c, Figure~\ref{fig:ds:rbfdrift}d is more dense. Figure~\ref{fig:exp:speedxaccu}a suggests that maximum deviation occurs within a values 0.01 and 0.1. Again, this is implementation specific value, depending on current MOA implementation.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of drift coefficient on accuracy (a) random RBF (b) VS RBF}
        \label{fig:exp:speedxaccu}
    \end{center}
\end{figure}

For random RBF stream, accuracy drops significantly for all algorithms with the introduction of drift in the system. In this range, bagging using ASHT and SRHT are the only two algorithms where no drift and very small drift has insignificant (~2\%) effect. Accuracy of adaptive HT based approaches drops the least, around 20\% while HT suffers greatly with more than 30\% drop in accuracy. Performance improvement over bagging using ASHT with bagging using SRHT is insignificant (within 1-2\%).
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of drift coefficient on processing (a) random RBF (b) VS RBF}
        \label{fig:exp:speedxtime}
    \end{center}
\end{figure}

For variable speed RBF stream, this has less importance as highest number of data are being produced by few centroids. The performance relies more on those specific centroids than all centroids' drift. Thus, accuracy remains in the similar level with varying drift. Importantly though, there is a significant improvement in accuracy after drift introduction, contrary to its random RBF counterpart. One plausible explanation would be, with drift the fewer centroids with higher data volume would cover more space, and have higher decision rules, thus achieving higher performance.

Next, on Figure~\ref{fig:exp:speedxtime} comparison of processing time for different drift coefficient is shown. As expected, classifier that maintains higher number of trees, complete (SRHT bagging) or sub-tree (adaptive HTs), take longer time with drifting data. The reason is with drifting concepts, more alternate trees or bigger trees need to be maintained. For the same reason tree depth, tree size, and memory shows similar trends as time plot (see Appendix~\ref{appndx:plots}).


\subsection{Effect of Number of Centroids}
\subsection{Effect of Percentage of Drifting Centroids}

\subsection{Effect of Grace Period}
\subsection{Effect of Tie Threshold}
\subsection{Effect of Binary Split}

\subsection{Effect of Ensemble Size}

\subsection{Effect of First Classifier}
\subsection{Effect of Reset vs Pruning}

\section{Timed Analysis}

