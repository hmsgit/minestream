\chapter{Experimental Evaluation}
\label{chp:exp}
In this chapter we present the empirical analysis of the discussed algorithms. We analyze the performances and properties of these algorithms varying a wide range of parameters. We compare our approach described in Chapter~\ref{chp:algo} with the current state-of-the-art methods presented in Chapter~\ref{chp:background}. Each of the analysis is performed simultaneously-- for both data streams generated by random RBF generator and our newer approach that generates speed varied RBF streams (Chapter~\ref{chp:dataset}). A 64-bit Windows 7 machine with Intel Core i7 dual-core processor clocking up to 2.4 GHz and 8 GB of RAM is used to run the experiments. The implementation is derived from Java based MOA framework~\cite{bifet:moa10}. 


\section{Evaluation Process}
There are two different approaches in evaluating algorithms in a streaming environment: (i) holdout evaluation, and (ii) prequential evaluation. In holdout evaluation, two separate train and test sets are used. Once the training is done, typically using one single-pass, on the training set, the test set is used to evaluate the performance. This is ideal where there are large amounts of data, where it is expected that with splitted test set, chances of over-fitting would be very low. In prequential evaluation, each example is first used to test the model in hand, and then the instance is used to train (update) the model. This method, thus, continuously changes the observed performance metrics and the model. A prequential model is, particularly, a necessity when the classifier has to ensure that the model is updated for the newest instances. The well-known cross validation approach is trivially not applicable in the streaming environment. In our settings, prequential evaluation is most appropriate. Binary class data sets of 1,000,000 instances in a 10 dimensional hyperspace are generated using the random RBF generator and the variable speed RBF generator for the evaluation. 

\subsection{Performance Metrics}
A number of metrics has been used for the evaluation criteria. Accuracy, processing time, memory usage, and kappa statistics are primarily used for the evaluation along with various tree structure related parameters such as tree depth, tree size, number of decision nodes, etc. Moreover, we observed algorithm specific properties e.g. number of tree reset, number of pruned trees, number of maintained alternate trees, number of times of partial tree replacement, etc. These algorithm specific parameters are not always directly comparable. Parameters are comparable among HT, adaptive HT, and ASHT. Bagging and boosting approaches have a different setting. Thus, we used weighted averages for these methods when calculating the number of resets or pruning type of statistics. For example, in bagging methods, smaller trees reset more often, thus while calculating the number of resets, a lower relative weight is given to the smaller trees. In our experiments, weights are proportional to the maximum size limits, or equal where there is no size restriction.

\subsection{Generalization Error Bound}
In batched leaning methods, the generalization error is a measurement that indicates how well a learning method generalizes to unseen data. It is the distance between the error on the training set and the test set, and is averaged over the entire set of possible training data for all iterations. In our stream mining environment, as we will be using prequential evaluation method, all samples would be first used as a test sample and then be used as a training sample. Moreover, as this will be performed only once, thus there would be no iteration over the training set, and the evaluation performance would be strongly tied with the sequence of  the incoming data. Thus, the notion of generalization error does not apply here. Rather, we focus on the confidence of the model being build. For Hoeffding tree based approaches, $\delta$ (Algorithm~\ref{alg:vfdt}, Algorithm~\ref{alg:srht}) is used to indicate an allowable error in each split decision. For our experiments, we kept this value to $0.0000001$. This is the maximum error to be accepted in a split decision. This is used to compute the Hoeffding bound, depending on which split decisions are taken.

\section{Study case: Census Income Dataset}
In the beginning, we check the performance of the stream algorithms in comparison to the classical batched data mining algorithms. For this evaluation, the census income data set~\cite{ron:adultds} is  used. This data set contains 48,842 instances of 14 attributes (6 continuous, 8 categorical) mapping to a binary ``income'' class (details in Appendix~\ref{appndx:ci}). With experimental evaluation, it is found that most batched learning methods such as C4.5, SVM, MLP achieve an accuracy around 82-83\% with 10 fold cross validation for the income class prediction task. As it can be seen from Figure~\ref{fig:exp:ci}, stream learning methods also achieve an accuracy within 1-2\% percent difference than the batched learners. Measured F1 values also presents similar scores. Though differences are insignificant, however, ASHT and bagging with ASHT seem to perform relatively worse than other algorithms. It is because of the resetting of the trees. After each reset, a few instances will suffer because the new trees are still very small. On the other hand, with our proposed modifications, bagging with SRHT overcomes this problem, and performs nearly the same as C4.5.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-3mm}\resizebox{75mm}{!}{\includegraphics{res/{0-ci-algo-accu}.pdf}} &
            
            \hspace{-5mm}\resizebox{75mm}{!}{\includegraphics{res/{0-ci-algo-fm}.pdf}} \\
            \scriptsize{(a)\vspace{2mm}} &
            \scriptsize{(b)}    
        \end{tabular}
        \caption{Comparison of batch and stream learning algorithms on census income data set with (a) accuracy and (b) F1-measure }
        \label{fig:exp:ci}
    \end{center}
\end{figure}


In terms of the tree structure, the final classifiers from a stream learner are simpler than the C4.5 tree. While C4.5 has many decision rules with over 10 decision nodes, Hoeffding tree based approaches are found to be performing similar with only 4 decision nodes. It is to note here that with a different ordering of the input data, a higher number of decision nodes is a possibility.

Lastly, we simulated a concept evolving scenario by controlled sampling of data from different clusters within the census income data. Five clusters were found using batched learning methods. While it had no significant effect on the performance of HT, however, methods that uses ADWIN are found to  get reset often depending on the sampling sequence. When each clusters is used sequentially, they reset all 4 times when concepts were changed.


\section{Impact of Parameters}
The evaluated algorithms depends on a number of parameters. Grace period, tie threshold, and tree type (binary or non-binary) are the most important parameters for HT, adaptive HT, and ASHT. For bagging and boosting methods the ensemble size is important. Additionally, for ASHT and SRHT, we have first classifier size, choice between reset and pruning are also important. Similarly, there are parameters on which the data generation processes (Chapter~\ref{chp:dataset}) rely on. In this section, we analyze the effect of each of these parameters starting with the parameters for data generation. For the discussion here, we used the most relevant metrics in respect to parameter. A comprehensive list of plots is given in the Appendix~\ref{appndx:plots} detailing all related metrics.

\subsection{Effect of Drift Coefficient}
Figure~\ref{fig:exp:speedxaccu}a suggests that the maximum deviation occurs within a value between 0.01 and 0.1. This is an implementation specific value, depending on the current MOA implementation. This is an indication that the drift coefficient changes the generation process periodically. That means after a threshold close to 0.05 drift reaches an amount that it starts forming clusters again. This agrees with previously seen cases. In Figure~\ref{fig:ds:rbfdrift} it was shown how generated data is distributed on a 2D space. Compared to Figure~\ref{fig:ds:rbfdrift}c, Figure~\ref{fig:ds:rbfdrift}d is denser.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{1-rnd-speed-accu}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{1-vs-speed-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of drift coefficient on accuracy using (a) random RBF (b) VSRBF}
        \label{fig:exp:speedxaccu}
    \end{center}
\end{figure}

For the random RBF stream, accuracy drops significantly for all algorithms with the introduction of drift in the system. In this range, bagging using ASHT and SRHT are the only two algorithms where no drift and very small drift have an insignificant (about 2\%) change. The accuracy of adaptive HT based approaches drop  the least, around 20\%; while HT suffers greatly with more than 30\% drop in accuracy. Performance improvement over bagging using ASHT with bagging using SRHT is insignificant (within 1-2\%).
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{1-rnd-speed-time}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{1-vs-speed-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of drift coefficient on processing time using (a) random RBF (b) VSRBF}
        \label{fig:exp:speedxtime}
    \end{center}
\end{figure}

For a variable speed RBF stream, this has less importance as the highest number of data is produced by a few centroids. The performance relies more on those specific centroids than on all centroids' drift. Thus, accuracy remains on the similar level with varying drift. Importantly though, there is  improvement in accuracy after drift introduction, contrary to its random RBF counterpart. One plausible explanation would be, with  the fewer drift centroids with higher data volume, it would cover more space, have higher decision rules, and thus achieve higher performance.

Next, on Figure~\ref{fig:exp:speedxtime} comparison of processing time for different drift coefficient is shown. As expected, classifiers that maintains higher number of trees, complete (SRHT bagging) or sub-trees (adaptive HTs), take longer time with drifting data. The reason is with drifting concepts, more alternate trees or bigger trees are needed to be maintained. For the same reason the tree depth, tree size, and memory show similar trends as the time plot (see Appendix~\ref{appndx:plots}).


\subsection{Effect of Number of Centroids}
The umber of generating centroids is another parameter for data generation. Each centroid in some way simulates concepts. We varied this number from 50 to 200 with steps of 50. No changes in terms of performance metrics is observed for random RBF data sets (Figure~\ref{fig:exp:centxaccu}a,~\ref{fig:exp:centxsize}a). For VSRBF streams a slight variation is observed, as depicted in Figure~\ref{fig:exp:centxaccu}b,~\ref{fig:exp:centxsize}b.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{3-rnd-centroid-accu}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{3-vs-centroid-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of number of centroids on accuracy using (a) random RBF (b) VSRBF}
        \label{fig:exp:centxaccu}
    \end{center}
\end{figure}

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{3-rnd-centroid-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{3-vs-centroid-tsize}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of number of centroids on tree size using (a) random RBF (b) VSRBF}
        \label{fig:exp:centxsize}
    \end{center}
\end{figure}

Tree sizes for all these algorithms fall into 3 general categories. Classifiers with size limitations (ASHT, SRHT) lies at the bottom of the plot, classifiers without any alternate tree maintenance scheme have higher tree sizes, however, those with alternate trees are about 5 times larger in sizes than these classifiers.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{4-rnd-driftcentroid-accu}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{4-vs-driftcentroid-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of percentage of drift centroid on accuracy using (a) random RBF (b) VSRBF}
        \label{fig:exp:driftxaccu}
    \end{center}
\end{figure}
\subsection{Effect of Percentage of Drifting Centroids}


In this set of experiments, we checked the effect of number of drifting centroids. Previously, we have shown the performance for different drifting amounts, and have found that with a relatively higher drifting rate (e.g. 0.01), performances of the algorithms drops substantially.  (Figure~\ref{fig:exp:speedxaccu}). For these experiments, we therefore, kept the drifting coefficient to 0.01 and varied percentage or drifting centroids from 20 to 100. As expected, accuracy and other metrics are found to be highly correlated with the percentage of drifting centroids till it reaches the random guess accuracy of 50\% (binary class problem). With 50\% of the centroids drifting with a coefficient of 0.01, performance reaches the minimum as shown in the Figure~\ref{fig:exp:driftxaccu}.


\subsection{Effect of Grace Period}
The grace period is the number of instances each node must receive before starting to check for split attribute. Thus, it is expected that with a higher grace period the algorithms will perform faster producing less decision nodes.


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{2-rnd-grace-accu}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{2-vs-grace-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of grace period on accuracy using (a) random RBF (b) VSRBF}
        \label{fig:exp:gracexaccu}
    \end{center}
\end{figure}



\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{2-rnd-grace-time}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{2-vs-grace-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of grace period on processing time using (a) random RBF (b) VSRBF}
        \label{fig:exp:gracextime}
    \end{center}
\end{figure}



\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{2-rnd-grace-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{2-vs-grace-tsize}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of grace period on tree size using (a) random RBF (b) VSRBF}
        \label{fig:exp:gracexsize}
    \end{center}
\end{figure}

We varied the grace period from 200 (MOA default) to 3200 in a geometric rate of 2. Results for accuracy, processing time, and tree size are shown in Figure~\ref{fig:exp:gracexaccu}, \ref{fig:exp:gracextime}, and \ref{fig:exp:gracexsize} respectively. These figures show that for both random RBF and variable speed RBF, the effect of grace period is similar. Processing time and tree size drop  about 25\% to 33\% while the effect on accuracy is negligibly within 0-2\% only.


\subsection{Effect of Tie Threshold}
The tie threshold is used to break ties between two similarly good attributes for a split decision. If their gain difference in not greater than the Hoeffding margin, but the margin itself is less than tie threshold then a node is splitted on the best attribute at hand.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{5-rnd-tiethresh-accu}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{5-vs-tiethresh-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of tie threshold on accuracy using (a) random RBF (b) VSRBF}
        \label{fig:exp:tiexaccu}
    \end{center}
\end{figure}

A larger value for the tie threshold ensures that the margin between the best two attributes is higher. As Figure~\ref{fig:exp:tiexaccu} shows that the larger values yields better accuracy when we varied tie threshold from 0.001 to 0.1 with discrete intervals. However, in Figure~\ref{fig:exp:tiexsize}, it can be seen that this also increases the tree size substantially for adaptive HT based methods. Both random RBF and VSRBF show similar trends, but classifiers perform better on VSRBF. The reason is explained in the section above about effect of the drift coefficient. We used 0.01 as drift coefficients for these experiments.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{5-rnd-tiethresh-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{5-vs-tiethresh-tsize}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of tie threshold on tree size using (a) random RBF (b) VSRBF}
        \label{fig:exp:tiexsize}
    \end{center}
\end{figure}

\subsection{Effect of Binary Split}
This set of experiments explores the effect of a binary split. This attribute has more importance in terms of tree structure than on the overall performance. The overall accuracy difference is negligible (about 1-2\%) for the non-binary and the binary tree. However, depth and number of decision nodes are greatly affected by this parameter. For example, tree sizes of adaptive HT based approaches drop  down from around 5000 nodes to under 1000 nodes when a binary splitting is used. Similarly, depths fall from around 25 to just 10. In Figure~\ref{fig:exp:binaryxaccu} we have shown the depths and tree sizes for a binary and non-binary split scenario for the random RBF generator. The variable speed RBF generator also produces similar results (see Appendix~\ref{appndx:plots}).

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{6-rnd-binsplit-depth}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{6-rnd-binsplit-tsize}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of binary split on (a) depth (b) tree size}
        \label{fig:exp:binaryxaccu}
    \end{center}
\end{figure}


\subsection{Effect of Ensemble Size}
The ensemble method has very little influence on static concepts. Figure~\ref{fig:exp:ensizexstatic} shows that for both random RBF and VSRBF generators increasing ensemble size achieves no better accuracy. But processing time increases linearly with increasing size.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{8-rnd-ensize-accu}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{8-vs-ensize-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of ensemble size for static concepts on  (a) accuracy (b) processing time}
        \label{fig:exp:ensizexstatic}
    \end{center}
\end{figure}

However, with a small drift coefficient of 0.001, algorithms demonstrate  higher accuracy with the increasing ensemble size (Figure~\ref{fig:exp:ensizexdrift}). 


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{8-rnd-ensize-accu-copy}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{res/{8-rnd-ensize-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of ensemble size for drifting concepts on  (a) accuracy (b) processing time}
        \label{fig:exp:ensizexdrift}
    \end{center}
\end{figure}

\subsection{Other Parameters}
Experiments are also performed for the maximum allowed size, the size of the first tree in an ensemble (other are multiples of the first one), and the reset vs pruning approaches. Their effect in our settings is found to be negligible. Evaluation plots are given in Appendix~\ref{appndx:plots}.


\section{Comparative Analysis of Timeline}
In the last section, we have presented performance metrics of the algorithms in terms of their parameters, and compared the algorithms based on the overall performance. However, with the final output of performances, we do not get the idea of how a tree reset or pruning affects the classifiers. Moreover, it is unclear that how bagging with SRHT is an improvement over bagging with ASHT. Thus, in this section, we present the analysis based on a windowed evaluation in a timeline manner. For this, we have used a windowed evaluation approach with 50,000 as the size of the window. Performances are sampled at each 50,000 instances. For this set of experiments, the used parameter values are: drift coefficient 0.001, grace period 200, tie threshold 0.05, percentage of drifting coefficient 100, ensemble size 5, binary split, and resettable. 

\subsection{Performance Over Time}

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-rnd-count-accu}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-vs-count-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Accuracy over time for (a) Random RBF (b) VSRBF}
        \label{fig:exp:taccu}
    \end{center}
\end{figure}


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-rnd-count-depth}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-vs-count-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Tree depth over time for (a) Random RBF (b) VSRBF}
        \label{fig:exp:tdeth}
    \end{center}
\end{figure}

Figure~\ref{fig:exp:taccu} shows the accuracy of the algorithms in different time of the window. Figure~\ref{fig:exp:tdeth} presents the depths of the trees at those corresponding times. For the random RBF generator, a lot of fluctuations in both accuracy (Figure~\ref{fig:exp:taccu}a) and tree depth (Figure~\ref{fig:exp:tdeth}a) can be seen. For the VSRBF generator, these fluctuations are bigger. The reason is presumably the high number of instances being generated from small number of concepts in VSRBF, and those concepts are constantly disappearing. However, an important thing to note here is the relationship between the drop in the depth and the drop in the accuracy for the random RBF generator, especially the bagging with a ASHT classifier. On the other hand, bagging with SRHT achieves a bit higher accuracy with a more stable classification rate. For VSRBF generator, bagging with ASHT seems to perform a little better than SRHT bagging but SRHT bagging is more stable. Some other classifiers are performing better (Figure~\ref{fig:exp:taccu}b), however, lead to an over-fitting scenario as the tree depth is constantly rising (Figure~\ref{fig:exp:tdeth}b).

This is essentially the goal of our new approach: to have a more stable ensemble such that deleting a classifier will not significantly affect accuracy for a certain period of time. Moreover,  Figure~\ref{fig:exp:pool} shows that bagging with SRHT improves performances for slower concepts (see Figure~\ref{fig:ds:varspd}).

\begin{figure}[htbp] 
    \begin{center}
            \resizebox{130mm}{!}{\includegraphics{resw/{0-vs-pool-accu}.pdf}}
            
        \caption{Accuracy within each data generating pool}
        \label{fig:exp:pool}
    \end{center}
\end{figure}

Among other metrics, the number of reset count found to be linearly increasing with time for both generators (Figure~\ref{fig:exp:treset}). Similarly, the processing time shows a linearly increasing trend over time (Figure~\ref{fig:exp:ttime}). Kappa statistics shows higher variations for VSRBF than for random RBF (Figure~\ref{fig:exp:tkappa}). 

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-rnd-count-reset}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-vs-count-reset}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Number of tree being reset over time for (a) Random RBF (b) VSRBF}
        \label{fig:exp:treset}
    \end{center}
\end{figure}

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-rnd-count-time}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-vs-count-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Processing time over time for (a) Random RBF (b) VSRBF}
        \label{fig:exp:ttime}
    \end{center}
\end{figure}


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-rnd-count-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{85mm}{!}{\includegraphics{resw/{1-vs-count-kappa}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Kappa over time for (a) Random RBF (b) VSRBF}
        \label{fig:exp:tkappa}
    \end{center}
    %\vspace{-5mm}
\end{figure}


\section{Discussion on the Experimental Evaluation}
We started this chapter by comparing stream learners with state-of-the-art batched learners. We showed that for the census income data set~\cite{ron:adultds} of about 49 thousand instances stream learners perform close to the best accuracies obtained by batched learners. This, however, cannot be used to conclude that stream learners would always show such a performance for other data sets.

Next, we have analyzed each of the algorithms including our new approach by varying the parameters within a wide range. We also have presented findings on the effect of different parameters of the data generation processes. It was found that the drift coefficient is inversely related with accuracy for 0 to 0.01, after which accuracy starts to increase again. Hence, this indicates that its effect on the data generation process is periodic. Performances of the learners are found to be nearly independent of the number of centroids used for a fairly high drift of 0.01. On the other hand, the percentage of drift centroids is inversely related to performance, and can bring accuracy down to the 50\% for a binary class problem with only 50\% drifting centroids. After that, increasing number of drifting centroids cannot not affect the classifier performance. 

From classifier parameters, a grace period is found to be very effective in reducing processing time and tree size, sacrificing a negligible amount of accuracy, which is essentially the motive to use a grace period. The use of higher threshold achieves higher accuracy while considerably increasing the tree size. It has also been shown that binary tree can achieve same accuracy but reducing tree size significantly. For ensemble based approaches, the ensemble size is found to be effective in drifting conditions while for the static conditions it had no effect. Other parameters have very little impact in our experimental setup.

Finally, we have presented the windowed performance evaluation or the timeline of performances of algorithms. It is here, where we can clearly distinguish and effectively prove the effectiveness of our new approach. While analyzing parameters, we have seen that bagging SRHT mostly performs better than bagging with ASHT. However, with the windowed analysis, we demonstrated how in ASHT bagging ensemble performs poorly after the trees being reset. Experimental results showed that SRHT bagging overcomes this issue and keeps a stable classifier though-out the time. Additionally, a breakdown of performance for different pools evidentially proved that SRHT is more effective for slower streams. Thus SRHT fairly realizes the motivation we presented at the beginning.
