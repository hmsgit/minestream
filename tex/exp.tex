\chapter{Experimental Evaluation}
\label{chp:exp}
In this chapter we present the empirical analysis of the discussed algorithms. We analyze the performances and properties of these algorithms varying wide range of parameters. We compare our approach described in Chapter~\ref{chp:algo} with the current state-of-the-art methods presented in Chapter~\ref{chp:background}. Each of the analysis is performed simultaneously-  for both data streams generated by random RBF generator and our newer approach that generates speed varied RBF streams (Chapter~\ref{chp:dataset}). A 64-bit Windows 7 machine with Intel Core i7 dual-core processor clocking up to 2.4 GHz and 8 GB of RAM is used to run the experiments. Primary implementation is derived from Java based MOA framework~\cite{bifet:moa10}. 


\section{Evaluation Process}
There are two different approaches in evaluating algorithms in streaming environment: (i) holdout evaluation, and (ii) prequential evaluation. In holdout evaluation, two separate train and test sets are used. Once the training is done, typically using one-single pass, using the training set, test set is used to evaluate the performance. This is ideal where there are large amount of data, where it is expected that with splitted test set, chances of over-fitting would be very low. In prequential evaluation, each example is used to test the model in hand first, and then the instance is used to train (update) the model. This method, thus, continuously changes the observed performance metrics and the model. Prequential model is, particularly, a necessity when the classifier has to ensure that the model is updated for the newest instances. Well-known cross validation approach is trivially not applicable in the streaming environment. In our settings, prequential evaluation is most appropriate. Binary class data sets of 1,000,000 instances in a 10 dimensional hyperspace are generated using random RBF generator and variable speed RBF generator for the evaluation. 

\subsection{Performance Metrics}
A number of metrics has been used for the evaluation criteria. Accuracy, processing time, memory usage, and kappa statistics are primarily used for the evaluation along with various tree structure related parameters such as tree depth, tree size, number of decision nodes, etc. Moreover, we observed algorithm specific properties e.g. number of tree reset, number of pruned trees, number of maintained alternate trees, number of times of partial tree replacement, etc. These algorithms specific parameters are not always directly comparable. Parameters are comparable among HT, adaptive HT, and ASHT. Bagging and boosting approaches have a different setting. Thus, we used weighted averages for these methods when calculating the number of reset or pruning type of stats. For example, in bagging methods, smaller trees reset more often, thus while calculating number of resets, a lower relative weight is given to the smaller trees. In our experiments, weights are proportional to the maximum size limits, or equal where there is no size restriction.


\section{Study case: Census Income Dataset}
To begin with, we check the performance of the stream algorithms in comparison to the classical batched data mining algorithms. For this evaluation, census income data set~\cite{ron:adultds} is  used. This data set contains 48,842 instances of 14 attributes (6 continuous, 8 categorical) mapping to a binary ``income'' class (details in Appendix~\ref{appndx:ci}). Most batched learning methods such as C4.5, SVM, MLP achieve an accuracy around 82-83\% with 10 fold cross validation for the income class prediction task. As it can be seen from Figure~\ref{fig:exp:ci}, stream learning methods also achieve an accuracy within 1-2\% percent difference than the batched learners. Measured F1 value also presents similar scores. Though margins are insignificant, however, ASHT and bagging with ASHT seem to perform relatively worse than other algorithms. It is evidently because of the resetting of the trees. After each reset, few instances will suffer because the new trees are still very small. On the other hand, with our proposed modifications, bagging with SRHT overcomes this problem, and performs nearly same as C4.5.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-3mm}\resizebox{75mm}{!}{\includegraphics{res/{0-ci-algo-accu}.pdf}} &
            
            \hspace{-5mm}\resizebox{75mm}{!}{\includegraphics{res/{0-ci-algo-fm}.pdf}} \\
            \scriptsize{(a)\vspace{2mm}} &
            \scriptsize{(b)}    
        \end{tabular}
        \caption{Comparison of classical and stream learning algorithms with census income data set (a) accuracy and (b) F1-measure }
        \label{fig:exp:ci}
    \end{center}
\end{figure}


In terms of the tree structure, final classifier tree from stream learner are simpler than the C4.5 tree. While C4.5 has many decision rules with over 10 decision nodes, Hoeffding tree based approaches are found to be performing similar with only 4 decision nodes. It is to be mentioned here that with different ordering of the input data, a higher number of  decision node count is a possibility.

Lastly, we simulated a concept evolving scenario by controlled sampling of data from different clusters within the census income data. Five clusters were found using batched learning methods. While it had no significant effect on the performance of HT, however, methods that uses ADWIN is found to  get reset often depending on the sampling sequence. When each clusters were used sequentially, they reset all 4 times when concepts were changed.


\section{Parameter Analysis}
The evaluated algorithms depends on a number of parameters. Grace period, tie threshold, and binary tree or not are the most important parameters for HT, adaptive HT, and ASHT. For bagging and boosting methods ensemble size is important. Additionally, for ASHT and SRHT, we have first classifier size, choice of reset or pruning are also important. Similarly, there are parameters on which the data generation processes (Chapter~\ref{chp:dataset}) rely on. In this section, we analyze the effect of each of these parameters starting with parameters for data generation. For the discussion here, we used most relevant metrics related to the each parameters. A comprehensive list of plots are given in the Appendix~\ref{appndx:plots} detailing all related metrics.

\subsection{Effect of Drift Coefficient}
Figure~\ref{fig:exp:speedxaccu}a suggests that maximum deviation occurs within a values 0.01 and 0.1. This is implementation specific value, depending on current MOA implementation. This is a indication that drift coefficient changes the generation process periodically. That is after a threshold close to 0.05 drift reaches an amount that it starts forming clusters again. This agrees with previously seen case. In Figure~\ref{fig:ds:rbfdrift} it was shown that how generated data is distributed on a 2D space. Compared to Figure~\ref{fig:ds:rbfdrift}c, Figure~\ref{fig:ds:rbfdrift}d is more dense.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of drift coefficient on accuracy (a) random RBF (b) VS RBF}
        \label{fig:exp:speedxaccu}
    \end{center}
\end{figure}

For random RBF stream, accuracy drops significantly for all algorithms with the introduction of drift in the system. In this range, bagging using ASHT and SRHT are the only two algorithms where no drift and very small drift have insignificant (~2\%) change. Accuracy of adaptive HT based approaches drop  the least, around 20\%; while HT suffers greatly with more than 30\% drop in accuracy. Performance improvement over bagging using ASHT with bagging using SRHT is insignificant (within 1-2\%).
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of drift coefficient on processing (a) random RBF (b) VS RBF}
        \label{fig:exp:speedxtime}
    \end{center}
\end{figure}

For variable speed RBF stream, this has less importance as highest number of data are being produced by few centroids. The performance relies more on those specific centroids than all centroids' drift. Thus, accuracy remains in the similar level with varying drift. Importantly though, there is  improvement in accuracy after drift introduction, contrary to its random RBF counterpart. One plausible explanation would be, with  the fewer drift centroids with higher data volume, it would cover more space, and have higher decision rules, thus achieving higher performance.

Next, on Figure~\ref{fig:exp:speedxtime} comparison of processing time for different drift coefficient is shown. As expected, classifier that maintains higher number of trees, complete (SRHT bagging) or sub-tree (adaptive HTs), take longer time with drifting data. The reason is with drifting concepts, more alternate trees or bigger trees are needed to be maintained. For the same reason tree depth, tree size, and memory shows similar trends as the time plot (see Appendix~\ref{appndx:plots}).


\subsection{Effect of Number of Centroids}
Number of generating centroids is another parameter for data generation. Each centroid in some way simulates concepts. We varied this number form 50 to 200 with step of 50. No changes in terms of performance metrics is observed for random RBF data sets (Figure~\ref{fig:exp:centxaccu}a,~\ref{fig:exp:centxsize}a). For VS RBF streams a slight variation is observed, as depicted in Figure~\ref{fig:exp:centxaccu}b,~\ref{fig:exp:centxsize}b.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{3-rnd-centroid-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{3-vs-centroid-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of number of centroid on accuracy (a) random RBF (b) VS RBF}
        \label{fig:exp:centxaccu}
    \end{center}
\end{figure}

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{3-rnd-centroid-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{3-vs-centroid-tsize}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of number of centroid on tree size (a) random RBF (b) VS RBF}
        \label{fig:exp:centxsize}
    \end{center}
\end{figure}

Tree sizes for all these algorithms are divided into 3 general groups. Classifiers with size limitations (ASHT, SRHT) lies at the bottom, classifiers without any alternate tree maintenance scheme have higher tree sizes, however, those with alternate trees are about 5 times larger in sizes than these classifiers.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{4-rnd-driftcentroid-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{4-vs-driftcentroid-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of percentage of drift centroid on accuracy (a) random RBF (b) VS RBF}
        \label{fig:exp:driftxaccu}
    \end{center}
\end{figure}
\subsection{Effect of Percentage of Drifting Centroids}


In this set of experiments, we checked the effect of number of drifting centroids. Previously, we have shown the performance for different drifting amount, and have found that with relatively higher drifting rate (e.g. 0.01), performances of the algorithms drops substantially.  (Figure~\ref{fig:exp:speedxaccu}). For these experiments, we therefore, kept the drifting coefficient to 0.01 and varied percentage or drifting centroids from 20 to 100. As expected, accuracy and other metrics are found to be highly correlated with the percentage of drifting centroids till it reaches the random guess accuracy of 50\% (binary class problem). With 50\% of the centroids drifting with a coefficient of 0.01, performance reaches the minima (Figure~\ref{fig:exp:driftxaccu}).


\subsection{Effect of Grace Period}
Grace period is the number of instances each node must receive before starting to check for split attribute. Thus, it is expected that with higher grace period the algorithms will perform faster producing less decision nodes.


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of grace period on accuracy (a) random RBF (b) VS RBF}
        \label{fig:exp:gracexaccu}
    \end{center}
\end{figure}



\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of grace period on processing time (a) random RBF (b) VS RBF}
        \label{fig:exp:gracextime}
    \end{center}
\end{figure}



\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-tsize}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of grace period on tree size (a) random RBF (b) VS RBF}
        \label{fig:exp:gracexsize}
    \end{center}
\end{figure}

We varied grace period from 200 (MOA default) to 3200 in a geometric rate of 2. Results for accuracy, processing time, and tree size are shown in Figure~\ref{fig:exp:gracexaccu}, \ref{fig:exp:gracextime}, and \ref{fig:exp:gracexsize} respectively. These figures show that for both random RBF and variable speed RBF, effect of grace period is similar. Processing time and tree size drop  about 25\% to 33\% while the effect on accuracy is negligibly within 0-2\% only.


\subsection{Effect of Tie Threshold}
Tie threshold is used to break ties between two similarly good attributes for a split decision. If their gain difference in not greater than the Hoeffding margin, but the margin itself is less than tie threshold then a node is splitted on the best attribute in hand.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{5-rnd-tiethresh-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{5-vs-tiethresh-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of tie threshold on accuracy (a) random RBF (b) VS RBF}
        \label{fig:exp:tiexaccu}
    \end{center}
\end{figure}

A larger value for tie threshold ensures that the margin between the best two attributes is higher. As Figure~\ref{fig:exp:tiexaccu} shows that the larger values yields better accuracy when we varied tie threshold from 0.001 to 0.1 with discrete intervals. However, in Figure~\ref{fig:exp:tiexsize}, it can be seen that this also increases the tree size substantially for adaptive HT based methods. Both random RBF and VS RBF show similar trend, but classifiers performs better on VS RBF. The reason is explained in the section above in effect of drift coefficient. We used 0.01 as drift coefficients for these experiments.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{5-rnd-tiethresh-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{5-vs-tiethresh-tsize}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of tie threshold on tree size (a) random RBF (b) VS RBF}
        \label{fig:exp:tiexsize}
    \end{center}
\end{figure}

\subsection{Effect of Binary Split}
This set of experiments explores the effect of binary split. This attribute has more importance in terms of tree structure than overall performance. Overall accuracy difference is negligible (about 1-2\%) for non-binary and binary tree. However, depth and number of decision nodes are greatly affected by this parameter. For example, tree sizes of adaptive HT based approaches drop  down from around 5000 nodes to under 1000 nodes when binary splitting is used. Similarly, depths fall from around 25 to just 10. In Figure~\ref{fig:exp:binaryxaccu} we have shown the depth and tree sizes for binary and non-binary split scenario for random RBF generator. Variable speed RBF also produces similar results. (See Appendix~\ref{appndx:plots} for other plots).

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{6-rnd-binsplit-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{6-rnd-binsplit-tsize}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of binary split on (a) depth (b) tree size}
        \label{fig:exp:binaryxaccu}
    \end{center}
\end{figure}


\subsection{Effect of Ensemble Size}
Ensemble method has very little influence on static concepts. Figure~\ref{fig:exp:ensizexstatic} shows that for both random RBF and VS RBF generators increasing ensemble size achieves no better accuracy. But processing time increases linearly with increasing size.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{8-vs-ensize-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of ensemble size for static concepts on  (a) accuracy (b) processing time}
        \label{fig:exp:ensizexstatic}
    \end{center}
\end{figure}

However, with a small drift coefficient of 0.001, algorithms demonstrate  higher accuracy with the increasing ensemble size (Figure~\ref{fig:exp:ensizexdrift}). 


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-accu-copy}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Effect of ensemble size for drifting concepts on  (a) accuracy (b) processing time}
        \label{fig:exp:ensizexdrift}
    \end{center}
\end{figure}

\subsection{Other Parameters}
Experiments are also performed for maximum allowed size, size of the first tree in a ensemble (other are multiples of first one), and reset vs pruning approaches. Their effect in our settings is found to be negligible. Evaluation plots are given in Appendix~\ref{appndx:plots}.


\section{Comparative Analysis Over Timeline}
In last section, we have presented performance metrics of the algorithms in terms of their parameters, and compared the algorithms based on the overall performance. However, with final output of performances, we do not get the idea of how tree reset or pruning affect the classifiers. Moreover, it is unclear that how bagging with SRHT is an improvement over bagging with ASHT. Thus, in this section, we present the analysis based on windowed evaluation in a timeline manner. For this, we have used windowed evaluation approach with 50,000 as the size of the window. Performances are sampled at each 50,000 instances. For this set of experiments, used parameter values are: drift coefficient 0.001, grace period 200, tie threshold 0.05, percentage of drifting coefficient 100, ensemble size 5, binary split, and resettable. 

\subsection{Performance Over Time}

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-rnd-count-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-vs-count-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Accuracy over time for (a) Random RBF (b) VS RBF}
        \label{fig:exp:taccu}
    \end{center}
\end{figure}


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-rnd-count-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-vs-count-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Tree depth over time for (a) Random RBF (b) VS RBF}
        \label{fig:exp:tdeth}
    \end{center}
\end{figure}

Figure~\ref{fig:exp:taccu} shows the accuracy of the algorithms in different time of the window. Figure~\ref{fig:exp:tdeth} presents depth of the trees at those corresponding times. For random RBF generator, a lot of fluctuations in both accuracy (Figure~\ref{fig:exp:taccu}a) and tree depth (Figure~\ref{fig:exp:tdeth}a) can be seen. For VS RBF generator, these fluctuations are  very radical. The reason is obviously the high number of instances being generated from small number of concepts in VS RBF, and those concepts are constantly disappearing. However, important thing to note here is the relationship between the drop in the depth and the drop in the accuracy for random RBF generator, especially the bagging with ASHT classifier. On the other hand, bagging with SRHT achieves bit higher accuracy with more stable classification rate. For VS RBF generator, bagging with ASHT apparently performs little better than SRHT bagging. But SRHT bagging is more stable. Some other classifiers are performing better (Figure~\ref{fig:exp:taccu}b), however, leading to an over-fitting scenario as the tree depth is constantly rising (Figure~\ref{fig:exp:tdeth}b).

This is essentially the goal of our new approach: to have a more stable ensemble such that deleting a classifier will not significantly affect accuracy for certain period of time. Moreover,  Figure~\ref{fig:exp:pool} shows that bagging with SRHT improves performances for slower concepts (see Figure~\ref{fig:ds:varspd}).

\begin{figure}[htbp] 
    \begin{center}
            \resizebox{130mm}{!}{\includegraphics{resw/{0-vs-pool-accu}.pdf}}
            
        \caption{Accuracy within pool}
        \label{fig:exp:pool}
    \end{center}
\end{figure}

Among other metrics, number of reset count found to be linearly increasing with time for both generators (Figure~\ref{fig:exp:treset}). Similarly, processing time shows linearly increasing trend over time (Figure~\ref{fig:exp:ttime}). Kappa statistics shows higher variations for VS RBF than for random RBF (Figure~\ref{fig:exp:tkappa}). 

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-rnd-count-reset}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-vs-count-reset}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Number of tree being reset over time for (a) Random RBF (b) VS RBF}
        \label{fig:exp:treset}
    \end{center}
\end{figure}

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-rnd-count-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-vs-count-time}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Processing time for (a) Random RBF (b) VS RBF}
        \label{fig:exp:ttime}
    \end{center}
\end{figure}


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-rnd-count-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{resw/{1-vs-count-kappa}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(b)} \\
            
        \end{tabular}
        \caption{Kappa for (a) Random RBF (b) VS RBF}
        \label{fig:exp:tkappa}
    \end{center}
\end{figure}
\clearpage


\section{Discussion}
We started this chapter by comparing stream learners with state-of-the-art batched learners. We showed that for census income data set of about 49 thousand instances stream learners performs close to the best accuracies obtained by batched learners. This, however, cannot be used to conclude that stream learners would always show such performances for other data sets.

Next, we have analyzed each of the algorithms including our new approach by varying the parameters within a wide range. We also have presented findings on effect of different parameters of data generation processes. It was found that drift coefficient is inversely related with accuracy for 0 to 0.01, after which accuracy starts increasing again. Hence, indicating that its effect on data generation process is periodic. Performances of the learners are found to be nearly independent of the number of centroids used for fairly high drift of 0.01. On the other hand, percentage of drift centroids is inversely related to performance, and can bring accuracy down to the 50\% for binary class problem with only 50\% drifting centroids. After that, increasing number of drifting centroids cannot not affect classifier performance. 

From classifier parameters, grace period is found to be very effective in reducing processing time and tree size sacrificing negligible amount of accuracy, which is essentially motive of using grace period. Use of higher threshold achieve higher accuracy while considerably increasing tree size. It has also been shown that binary tree can achieve same accuracy but reducing tree size significantly. For ensemble based approaches, ensemble size is found to be effective in drifting conditions while for static conditions had no effect. Other parameters have very little impact in our experimental setup.

Finally, we have presented the windowed performance evaluation or timeline of performances of algorithms. It is here, where we get to clearly differentiate and effectively prove the effectiveness of our new approach. While analyzing parameters, we have seen that bagging SRHT mostly performs better than bagging with ASHT. However, with windowed analysis, we demonstrated how in ASHT bagging ensemble performs poorly after trees being reset. Experimental results showed that SRHT bagging overcomes this issue and keeps a stable classifier though-out the time. Additionally, breakdown of performance for different pools evidentially proved that SRHT is more effective for slower streams. Thus SRHT fairly realizes the motivation we presented at the beginning.
