\chapter{Introduction}
\label{chp:intro}
Huge amount of data are generated in an unprecedented rate now-a-days from different application domains like social networks, telecommunications, WWW, scientific experiments, e-commerce systems, health care systems, etc. These data flow in and out of systems continuously with varying update rates. Banking systems keep registering each of their ATM and account transactions, telecommunication providers log each of their call information, popular websites maintain their user logging details, organization like CERN produces petabytes of data during their experiments; these are known as stream data. The volume and the rate of incoming data make it nearly impossible to mine these data with traditional data mining approaches. As alternatives, mining a subsample of available data or to mine for models much simpler than what the data might support can be performed. This, however, drastically limits the ability to extract information from the data. Moreover, in some cases, accumulating and storing the data in runtime and performing a sampling to apply mining algorithms simultaneously is a challenge. For such reasons the notion of mining fixed-size database is slowly being replaced with the idea of open-ended data streams.

Compared to the classical data mining approaches stream mining is relatively a newer topic to be addressed in literature. Even though for batched approaches both classification and clustering problems have been vastly studied, their stream adaptations remain a challenge due the restrictions imposed by the stream data. Due to the volume and the nature of the stream data there are several restrictions that are needed to be considered while designing a stream mining algorithm. Most important limitations include- not being able to store the complete data set in memory and hence only being able to use an example once to train the model; evolving nature of the data with time changes, etc. Thus stream mining requires different approach than the traditional batch learning. Possibility of temporal locality makes the classification problem even harder in a streaming environment. Algorithms need to address the evolution of underlying data stream.

Tradition machine learning algorithms generally feature a single model or classifier such as Na\"ive Bayes or multilayer perceptron (MLP) learned from the training set and use it to classify testing set. The free parameters of these learners (e.g. weights of feed-forward neural network) are set by realizing the complete training set. These classifiers provide a measurement of the generalized performance i.e. how well the classifiers generalize the training set. Some of the assumptions these algorithms make are: (i) data are finite, (ii) underlying data regularities are stationary, (iii) stationary data sources generate independent and identically distributed data, (iv) data are invariant to the spatial or temporal changes, etc. None of these assumptions is valid for data streams. Stream data exhibit following characteristics: 
\begin{itemize}
    \item Data come as a continuous flow of unlimited stream, often at a very high speed.
    \item Underlying models in the data evolve over time.
    \item Data cannot be considered to be independent and identically distributed.
    \item Time and space can significantly influence data.
\end{itemize}
At the first glance, it may seem that some simple adaptation of current methods should be sufficient to address these changed conditions in data. In reality, these changes challenge most trivial learning methods of machine learning. For example, the computation of entropy of data. For batched learning all the instances and their corresponding classes are known before-hand to compute entropy. However, for streaming case, data stream is no longer finite, number of classes are not known a priori, domain of variables are not necessarily small in size; and hence computation of batched-like entropy function is not possible. Similarly, maintaining a frequent item set in a hundred of gigabytes of database also cannot be easily be derived from traditional machine learning algorithms.

Typically, adaptations of traditional algorithms need to address continuous flow of data, dynamic environment of generating sources, unavailability of complete data set, etc. Following are some of the new requirements that are needed to be considered while developing a knowledge discovery system for stream data:
\begin{itemize}
    \item Algorithms should use limited resources in terms of power, memory, and processing time.
    \item Algorithms should only access the data a limited number of times, and may only use a limited bandwidth.
    \item Algorithms should be ready to predict {\it anytime}. 
    \item Data gathering and processing might be distributed.
\end{itemize}

With traditional algorithms, a single model is learned, i.e. only one single generalization of the data is learned. However, given a finite set of training example, it is rather reasonable to assume that the data might contain several generalizations. For example, a different setting of neural network classifier (weights, node layers, node counts, etc.) might change the final network to some extent. For stream environment this assumption becomes primitive. Thus, choosing a single classifier is not always optimal. Using the best classifier among several classifiers where each is trained with same training set would be an alternative, however, information are still being lost by discarding sub-optimal options. A better alternative would be to build a classifier ensemble. Ensemble classifiers combine the predictions of multiple base level models built on traditional algorithms. A simple process for combining prediction would be to choose the decision based on majority voting. As demonstrated in several works~\cite{breiman94:bagging, schapire90:whyens} such ensemble methods (e.g. ensembles of neural networks) yield better performance.

Without proper selection and control over the training process of the base learners, ensemble classifiers could result in poorer performance. Simply choosing a base classifier and training it for several settings would surely produce highly correlated classifiers which would have adverse effect on the ensemble process. One solution of this issue is to train each classifier with its own training set generated by random sampling over the original one. However, with random sampling each classifier would receive a reduced number of training patterns, resulting in a reduction in the accuracy of the individual base classifiers. This reduction in the base classifiers' accuracy is generally not recovered by the gain of combining the classifiers unless measures are taken to make the base classifiers diverse.

Diversity is generally achieved by making the base classifiers minimally correlated. In recent years, various methods have been developed to address this issue. Among others bagging, boosting, Hoeffding Tree based approaches are mentionable. Some of these approaches are suitable for label based classification, and some can be used to approximate the trend of data over a specific time granularity. It will, however, be nice if a method can approximate the trend of data over a set of time granularity. Moreover, investigating how ensemble methods perform where base classifiers are trained to predict one specific labels should be interesting. For example, given a twitter stream an ensemble of classifiers that may contain base classifiers to classify sentiments for sports, politics, entertainment, etc. separately. It is expected this ensemble setting would out-perform the generic ensemble approach for complete stream altogether.

Ensemble methods build model outputs where abstract properties of the algorithms that produces the model are prioritized rather than the specifics of the algorithms. This allows a wide application across many fields of study. With precise use of ensemble methods, it would be possible to automatically exploit the strengths and weaknesses of different machine learning systems.

This thesis investigates currently available such ensemble approaches, and presents an empirical analysis of those methods. Moreover, this thesis presents a unique perspective in relation to the underlying setup that generates the stream, which applies to certain application domains such as social media. In following sections, we present this motivation and probable approaches to address such scenarios.

%\newpage
\section{Motivation}
\label{sec:intro:motiv}
One of the major challenges faced in stream mining is the lack of labeled data. There is no such problem in batched leaning. Data are finite and only, if any, an insignificant portion might be unlabeled. Streaming scenarios show a stark contrast. Most of the real world data are mostly unlabeled. It needs human intervention, resources, and time to properly label a stream data set. Most often only a fraction of data set is labeled by human expert or automated scripts are used. Owing to the fact of having such limitations, a large number of experimentations for stream mining algorithms are performed using synthesized data. It is much easier to control different parameters, concept drift, labeling, etc. in a generated data set. In most of the processes, this data generation process is mostly randomized, and probability of data generating from a certain region remains the same for entire hyperspace. Temporal locality are sometimes created by adding bias to certain region, however, the rates at which these regions produce data points mostly remain the same for all the regions in the hyperspace.

In reality many practical scenarios do not follow a uniform distribution for data generation. Some regions are more active than others. The term ``more active'' used here is in the sense that they produce more data. For example, Figure~\ref{fig:intro:nettraffic} shows the Internet traffic on an average day. A reference heat-map scale shows the blue color to be less than average and red to be higher than average. As the figure shows based on the time of the day, amount of network traffic in each region changes drastically even though the number of active nodes remains almost the same.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \resizebox{70mm}{!}{\includegraphics{figs/nettraffic-a.png}} &
            \resizebox{70mm}{!}{\includegraphics{figs/nettraffic-b.png}} \\
            \scriptsize{(a)\vspace{2mm}} &
            \scriptsize{(b)}    
        \end{tabular}
        \caption{Worldwide Internet traffic. Day time in (a) western (b) eastern hemisphere \cite{internet:huffpost:nettraffic}}
        \label{fig:intro:nettraffic}
    \end{center}
\end{figure}

In another example, let's consider profiling of cell phone user based on the phone usages. Typically, young people are more inclined towards using data services than voice services while the professional and elderly people mostly rely on voice services. Thus, the profiling algorithm should consider this differences in rates in which different user groups are using the services.

To get a clearer picture of the locality of data activation and rate in which data are generated, let's consider the social media statistics. As of the second quarter of 2015, Facebook had 1.49 billion monthly active users. In the third quarter of 2012, the number of active Facebook users had surpassed 1 billion. Active users are those who log into Facebook during the last 30 days. It was only on the August 28, 2015 that Facebook had 1 billion user on a single day. Let assume that we want to do a sentiment analysis over the data set collected from a week of data of Facebook. Even though the class distribution (positive and negative sentiments)  may remain balanced, however, more active users will clearly overshadow the inputs given by the less active users. Similarly, about 300 million users produce 500 million tweets per day on Twitter. Figure~\ref{fig:intro:tweets} shows tweets for 5 different hashtags namely \#Trump, \#Syria, \#football, \#Volkswagen, and \#eclipse for the period of Aug 29, 2015 - Sept 28, 2015 (\cite{internet:topsy:tweets}). 
\begin{figure}[htbp] 
    \begin{center}
            \resizebox{110mm}{!}{\includegraphics{figs/tweets.pdf}}
        \caption{Number of tweets for different hashtags }
        \label{fig:intro:tweets}
    \end{center}
\end{figure}
\noindent As it can be seen, \#football has a weekly repetitive trend, most certainly because of weekend games. \#Trump (USA's presidential candidate for 2016), on the other hand, has a steady rate with occasional spikes depending on some highlighted events e.g. debate. Topic like \#Syria has lower rate, however, has a steady flow. Owing to the news of carbon emission from Volkswagen cars and lunar eclipse of September 28, 2015, topics like \#Volkswagen and \#eclipse got trending; which can be expected to disappear soon. If each of these topics is to be treated equally, slower streams like \#Syria would suffer from lack of data.

Based on the discussion above, it can evidently be seen that these streams do not necessarily follow a uniform distribution. Rather, these streams can be considered to be a collection of many sub-streams. Some properties of these sub-streams are follows:
\begin{itemize}
    \item A set of slow sub-streams generating low but relatively consistent number of data.
    \item A set of alternating sub-streams generating moderate number of data. Activation of sub-streams within the set is dependent to one other.
    \item A set of sub-streams that produces very large amount of data but only remain active for very short period of time.
\end{itemize}

Applying machine learning methods in such streams without considering the differences in the rate of data incoming for different sub-streams might lead to a set of decision rules dominated by the sub-streams with the highest number of instances. Moreover, most stream mining algorithms only keep track of most recent incoming instances and forget older data. Such algorithms would thus forget important decision rules learned over longer period of times for slow but consistent sub-streams when a heavy burst of occasional sub-streams arrives. It would take long time to learn the same rules again. Similar burst could lead to the deletion of decision rules for recurring sub-streams too. This could significantly affect the performance measures. Even for slow sub-streams overall performance measures may not reflect poor decision performances for those slow sub-stream, as the number of instances belonging to those streams are not high. But ideally it is expected that the mining algorithms should be equally effective for entire data space.

In this thesis, we address this scenario. To our best knowledge no previous work has specifically addressed this issue. In most works, evaluation are always performed with randomized streams. Concept drift, concept evolution, concept recurrence, etc. are addressed within the randomized streams. We extend one such data generation algorithm to implement the scenario presented above. Empirical evaluation are performed to compare the performances of existing algorithms for such streams. Comparing the results with the results from general randomized streams, it is found that existing algorithms do not perform at their best for streams with high temporal locality. Thus, this thesis presents an ensemble algorithm devised from one of the state-of-the-art algorithm to improve its performance. Extensive evaluation shows that new adaptation achieves more stable outcomes in overcoming the challenges posed by the nature of the stream. It also retains all the positive factors of the original algorithm.

Next section discusses the basic idea of the algorithm without going into mathematical details. We presented the algorithm later in great details once the related literatures and concepts are introduced in next couple of chapters.

\section{Intuition}
The central idea of our methods is based on the primal decision tree adaption by Domingos and Hulten~\cite{domingos00:vfdt} for streams named Very Fast Decision Tree (VFDT), also commonly known as Hoeffding Tree (HT). Their method is based on the assumption that to find the best attribute for a split decision at any node in a decision tree, it would be sufficient to consider only a certain amount of training data on that node. Hoeffding bound~\cite{hoeffding63:bound} is a measurement of degree of certainty for such approach, which gives an error bound for a decision taken after seeing a certain amount of instances. This bound essentially states that any decision taken after observing a certain amount of instances would remain the same after seeing an infinite number of instances and the error margin can be computed with this bound. Detailed discussion of this bound and the algorithm is presented in Chapter~\ref{chp:background}.

With such approach, challenges posed by high volume of data can be avoided. It does not require to remembering all the observed data instances. Rather, the statistics of the instances are sufficient to effectively decide about split attribute. One of the limitations of Hoeffding Tree is that it grows linearly as the time passes or instances arrives at the system. Root is the node that was splitted by observing the oldest set of examples. Similarly, decision rules at lower levels also become old, while the leaves contain the information from most recent instances. To keep the rules updated numerous approaches such as reseting the tree, pruning bad performing nodes, etc have been proposed. Detailed discussion of adaptations for concept drift, evolution, recurrence, etc. are being discussed in Chapter~\ref{chp:background}.

With reset and pruning facility Hoeffding Tree exhibits a special property. It always adapts itself for the newer examples. Number of examples HT's decision rules depend on is directly related to the size of the tree and number of examples required to split a node. Number of decision or split nodes could a measurement of the size of the tree. A smaller tree would adapt faster to the changes in the data. A larger tree would take longer time to adapt. Using this rationale a bagging method based of different sized trees is proposed by Bifet et al.~\cite{bifet09:asht}. In this method, a fixed number of Hoeffding Trees with different size (number of decision nodes) limits are used. Each time a tree exceeds the size threshold, the tree gets reset. Trivially, smaller trees would reset more often than the larger trees. And thus decision rules from smaller trees will base on the most recent data. Larger trees, however, would also have decision rules from older data.

We combined all these concepts to address our problem. First, we introduced a size restricted variant of Hoeffding Tree namely Size Restricted Hoeffding Tree (SRHT). Unlike its predecessor Adaptive Size Hoeffding Tree~\cite{bifet09:asht}, it does not get reset immediately after it reaches the size limit. Instead, it stops growing i.e. no further split occurs, however, the weights in the leaf nodes are updated with incoming instances. It also combines two different concepts introduces by its predecessors: (i) to reset once a size limit is reached and (ii) to maintain alternate trees or subtrees where necessary. Thus, even after reaching the size limit, a tree can switch to an alternate tree and start growing again till the limit is reached again. 

We use this setup for a modified bagging scheme introduced in~\cite{bifet09:asht}. Similar to the Hoeffding Tree, a smaller SRHT will have decision rules for most recent data, and a larger tree would contain rules for some older instances too. To put simply in our problem's context, smaller tree would have decisions for high speed sub-streams or burst of incoming streams. Whereas only the larger trees will have some decision rules for slow but consistent sub-streams, along with the decision rules for most recent data. This is because, the recent data are always dominated by high speed sub-streams. Smaller data do not get enough information about the slow stream or enough examples from slow streams to decide on them. Keeping this in mind, we control the reset of larger trees to keep hard-learned decision rules. In doing so, we maintain an alternate pool of trees that are to be reset soon, and start maintain a new tree with same size limit from scratch. For the transition time we consider votes from all the trees, thus effectively increasing the weights for slower streams. In Chapter~\ref{chp:algo}, we present more elaborate description of the algorithms.

\input{literature.tex}

\section{Thesis Accomplishments}
The primary objective of this thesis is to devise an algorithm realizing the intuitions discussed earlier. In doing so, an extensive survey of existing literatures is performed. The most relevant portion of which is already been presented in previous section. The rest is attached at the end of this thesis (see Appendix~\ref{appndx:erw}). This thesis presents a new way to look at the composition of various real-world streams e.g. social networks. Based on that, a new data generation of approach to facilitate slow, fast, burst, recurrent, etc. behavior in a randomized data stream is proposed. In an effort to combine the ideas of adaptive size Hoeffding tree and adaptive Hoeffding tree using ADWIN, a new ensemble method is devised aiming to improve performance for slower streams. Lastly, a comprehensive empirical comparison of current Hoeffding tree based approaches is performed and justification of the new approach is performed using synthesized data.


\section{Outline}
The rest of the article is organized as follows: Chapter~\ref{chp:background} presents the base concepts required for the algorithm. We first discuss the basic streaming adaptations of batched methods, and later their usages in ensemble approaches. Chapter~\ref{chp:algo} precisely defines the problem and discusses the development of our algorithm addressing the motivation and intuition of this chapter. In Chapter~\ref{chp:dataset} existing data set generators are discussed first and a new data generation scheme is explained that fulfills the requirements discussed in the intuition section. Finally, before concluding the thesis in Chapter~\ref{chp:conclude}, in Chapter~\ref{chp:exp} we detail the experimental evaluation process and findings. Extensive evaluation is performed to compare existing methods, as well as, method devised in this thesis  (Chapter~\ref{chp:algo}). 
