\chapter{Introduction}
\label{chp:intro}
Huge amounts of data are generated in an unprecedented rate now-a-days from different application domains like social networks, telecommunications, WWW, scientific experiments, e-commerce systems, health care systems, etc. These data flow in and out of systems continuously and with varying update rates. Banking system keeps registering each of their ATM and account transactions, telecommunication providers logs each of their call information, popular websites maintains their user logging details, organization like CERN produces petabytes of data during their experiments; these are known as stream data. Mining of stream data is relatively a newer topic as compared to the classical data mining. The volume and the rate of data incoming make is nearly impossible to mine these data with traditional data mining approaches. As alternatives, mining a subsample of available data or to mine for models much simpler than what the data might support can be performed. This, however, drastically limits the ability to extract information from the data. Moreover, in some cases, accumulating and storing the data in runtime and performing a sampling to apply mining algorithms itself is a challenge. For such reasons the notion of mining fixed-size database is slowly being replaced with the idea of open-ended data streams.

Compared to the classical data mining approaches stream mining is relatively a newer topic to be addressed in literature. Even though for batched approaches both classification and clustering problems have been vastly studied, their stream adaption remain a challenge due the restrictions imposed by the stream data. Due to the volume and the nature of the stream data there are several restrictions that are needed to be considered while designing a stream mining algorithm. Most important limitations includes not being able to store the complete dataset in memory and hence only being able to use an example once to train the model; evolving nature of the data with time, etc. Thus streams mining requires different approach than the traditional batch learning. Possibility of temporal locality makes the classification problem hard in a streaming environment. Algorithms needs to address the evolution of underlying data stream.

Tradition machine learning algorithms generally feature a single model or classifier such as Na\"ive Bayes or multilayer perceptron (MLP) learned from the training set and use it to classify testing set. The free parameters of these learners (e.g. weights of feed-forward neural network) are set by realizing the complete training set. These classifier provides a measurement of the generalization performance i.e. how well the classifier generalizes the training set. Few of the assumptions these algorithms make are: (i) data are finite, (ii) underlying data regularities are stationary, (iii) stationary data sources generate independent and identically distributed data, (iv) data are invariant to the spatial or temporal changes, etc. None of these assumptions is valid for data streams. Stream data exhibit following characteristics: 
\begin{itemize}
    \item Data come as a continuous flow of unlimited stream, often at a very high speed.
    \item Underlying models in the data evolve over time.
    \item Data cannot be considered to be independent and identically distributed.
    \item Time and space can significantly influence data.
\end{itemize}
At the first glance, it may seem that some simple adaptation of current methods, should be sufficient to address these changed conditions in data. In reality, these changes challenge most trivial learning methods of machine learning. Consider the computation of entropy of data. For batched learning all the instances and their corresponding classes are known before-hand to compute entropy. However, for streaming case, data stream is no longer finite, number of classes are not known a priori, domain of variables are not necessarily small in size; and hence computation of batched-like entropy function is not possible. Similarly, maintaining a frequent item set in a hundreds of gigabytes of data is also cannot be easily be derived from traditional machine learning algorithms.

Typically adaptation of traditional algorithms needs to address continuous flow of data, dynamic environment of generating sources, unavailability of complete dataset, etc. Following are some of the new requirements that are needed to be considered while developing a knowledge discovery system for stream data:
\begin{itemize}
    \item Algorithms should use limited resources in terms of power, memory, and processing time.
    \item Algorithms should only access the data certain limited number of times, and may only use limited bandwidth.
    \item Algorithms should be ready to predict {\it anytime}. 
    \item Data gathering and processing might be distributed.
\end{itemize}

With traditional algorithms, a single model is learned, i.e. only one single generalization of the data is learned. However, given a finite set of training example, it is rather reasonable to assume that the data might contain several different generalization. For example, a different setting of neural network classifier (weights, node layers, node counts, etc.) changes the final network to some extent. For stream environment this assumption becomes primitive. Thus, choosing a single classifier is not always optimal. Using the best classifier among several classifier where each are trained with same training set would be an alternative, however, information are still being lost by discarding sub-optimal options. A better alternative would be to build a classifier ensemble. Ensemble classifiers combine the prediction of multiple base level model built on traditional algorithm. A simple process for combining prediction could be to choose the decision based on majority voting. As demonstrated in several works~\cite{breiman94:bagging, schapire90:whyens} ensemble methods (e.g. ensembles of neural networks) yields better performance.

Without proper selection and control over the training process of the base learners, ensemble classifiers could result in poorer performance. Simply choosing a base classifier and training it for several settings would surely produce highly correlated classifiers which would have adverse effect on the ensemble process. One solution of this issue is to train each classifier with its own training set generated by sampling over the original one. However, with random sampling each classifier would receive a reduced number of training patterns, resulting a reduction in the accuracy of the individual base classifier. This reduction in the base classifier accuracy is generally not recovered by the gain of combining the classifier unless measures are taken to make the base classifiers diverse.

Diversity is generally achieved by making the base classifiers minimally correlated. In recent years, various methods have been developed addressing this issue. Among others bagging, boosting, Hoeffding Tree based approaches are mentionable. Some of these approaches are suitable for label based classification, and some can be used to approximate the trend of data over a specific time granularity. It will, however, be nice if a method can approximate the trend of data over a set of time granularity. Moreover, investigating how ensemble methods performs where base classifiers are trained to predict one specific labels should be interesting. For example, given a twitter stream an ensemble of classifiers may contain base classifiers to classify sentiments for sports, politics, entertainment, etc. separately. It is expected this ensemble setting would out-perform the generic ensemble approach for complete stream altogether.

Ensemble methods build model outputs where abstract properties of the algorithms that produces the model are prioritized rather than the specifics of the algorithms. This allows a wide application across many fields of study. With precise use of ensemble methods, it would be possible to automatically exploit the strengths and weaknesses of different machine learning systems.

This thesis investigates currently available such ensemble approaches, and presents an empirical analysis of those methods. Moreover, this thesis presents a unique perspective relating to the underlying setup that generates the stream, that applies certain application domains such as social media. In following sections, we present this motivation and probable approaches to address such scenarios.

%\newpage
\section{Motivation}
One of the major challenges faced in stream mining is the lack of labeled data. There is no such problem in batched leaning. Data are finite and only an insignificant portion might be unlabeled, if there is any. Streaming scenarios show a stark contrast. Most of real world data are mostly unlabeled. It needs human intervention, resources, and time to properly label a stream data set. Most often only a fraction of data set is labeled by human expert or automated scripts are used. Owing to the fact of having such limitations, a large number of experimentations for stream mining algorithms are performed using synthesized data. It is much easier to control different parameters, concept drift, labeling, etc. in a generated dataset. In most of the processes, this data generation process is mostly randomized, and probability of data generating from a certain region remains the same for entire hyperspace. Temporal locality are sometimes created by adding bias to certain region, however, the rates at which these regions produces data points are mostly remain the same for all the regions in the hyperspace.

In reality many practical scenarios does not follow a uniform distribution for data generation. Some regions are more active than others. The term ``more active'' used here is in the sense that they produces more data. For example, Figure~\ref{fig:intro:nettraffic} shows the Internet traffic on an average day. A reference heat-map scale shows the blue color to be less than average and red to be higher then average. As the figure shows based on the time of the day, amount of network traffic in each region changes drastically even though the number of active nodes remains almost the same.

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{c}
            \resizebox{100mm}{!}{\includegraphics{figs/nettraffic-a.png}} \\
            \scriptsize{(a)\vspace{2mm}} \\
            \resizebox{100mm}{!}{\includegraphics{figs/nettraffic-b.png}} \\
            \scriptsize{(b)}    
        \end{tabular}
        \caption{Worldwide Internet traffic. Day time in (a) western (b) eastern hemisphere \cite{internet:huffpost:nettraffic}}
        \label{fig:intro:nettraiffc}
    \end{center}
\end{figure}

In another example, lets consider profiling of cell phone user based on the phone usages. Typically, young people are more inclined towards using data services than voice services while the professional and elderly people mostly rely on voice services. Thus, if the profiling algorithm should consider this differences in rates in which different user groups are using the services.

To get a clearer picture of the locality of data activation and rate in which data are generated, let's consider the social media statistics. As of the second quarter of 2015, Facebook had 1.49 billion monthly active users. In the third quarter of 2012, the number of active Facebook users had surpassed 1 billion. Active users are those who have logged in to Facebook during the last 30 days. It was only on the August 28, 2015 that Facebook had 1 billion user on a single day. Let assume that we want to do a sentiment analysis over the data set collected from a week of data of Facebook. Even though the class distribution (positive and negative sentiments)  may remain balanced, however, more active users will clearly overshadow the inputs given by the less active users. Similarly, about 300 million user produces 500 million tweets per day on Twitter. Figure~\ref{fig:intro:tweets} shows tweets for 5 different hashtags namely \#Trump, \#Syria, \#football, \#Volkswagen, and \#eclipse for the month of September 2015. 
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{c}
            \resizebox{100mm}{!}{\includegraphics{figs/tweet-trump-football-syria.png}} \\
            \scriptsize{(a)\vspace{2mm}} \\
            \resizebox{100mm}{!}{\includegraphics{figs/tweet-trump-vw-eclipse.png}} \\
            \scriptsize{(b)}    
        \end{tabular}
        \caption{Number of tweets for different hashtags \cite{internet:topsy:tweets}}
        \label{fig:intro:tweets}
    \end{center}
\end{figure}
\noindent As it can be seen, \#football has a weekly repetitive trend, most certainly because of weekend games. \#Trump, on the other hand, has a steady rate with occasional spikes depending on some highlighted events e.g. debate. Topic like \#Syria has lower rate, however, has a steady flow. Owing to the news of carbon emission from Volkswagen cars and lunar eclipse of September 28, 2015, topics like \#Volkswagen and \#eclipse got trending. Which will also soon disappear.

Based on the discussion so far, it is evidently be seen that these streams does not necessarily follow a uniform distribution. Rather these streams can be considered to be a collection of many sub-streams. The nature of these sub-streams could be as follows (but not limited to):
\begin{itemize}
    \item A set of slow sub-streams generating low but relatively consistent number of data.
    \item A set of alternating sub-streams generating moderate number of data. Activation of sub-streams within the set are dependent to one other.
    \item A set of sub-streams that produces very large amount of data but only remain active for very short period of time.
\end{itemize}

Applying machine learning methods in such streams without considering the differences in the rate of data incoming for different sub-streams would lead to a set of decision rules dominated by the sub-streams with highest number of instances. Moreover, most stream mining algorithms only keep track of most recent incoming instances and forget older data. Such algorithms would thus forget important decision rules learned over longer times for slow but consistent sub-streams when a heavy burst of occasional sub-streams arrives. It would take long time to learn the same rules again. Similar burst could lead to the deletion of decision rules for recurring sub-streams too. This could significantly affect the performance measures. Even for slow sub-streams overall performance measures may not reflect poor decision performances for those slow sub-stream, as the number of instances belonging to those streams are not high. But ideally it is expected the mining algorithm should be equally effective for entire data space.

In this thesis, we address this scenario. To our best knowledge no previous work has specifically addressed this issue. The evaluation are always done with randomized streams. Concept drift, concept evolution, recurrence are addressed within the randomized streams. We extended one such data generation algorithm to implement the scenario present above. Empirical evaluation are presented to compare the performance of existing algorithms for such streams. Comparing the results with the results from general randomized streams, it is evident that existing algorithms do not perform at their best for streams with temporal locality. Thus, this thesis presents an ensemble algorithm devised from one of the state-of-the-art algorithm to improve performance. Extensive evaluation shows that new adaptation overcomes the challenges posed by the nature of the stream and retains all the positive factors of the original algorithm.

Next section discusses the basic idea of the algorithm without going into mathematical details. We presented the algorithm in great details once the related literatures and concepts are introduced in next couple of chapters.

\section{Intuition}
The central idea of our methods is based on the primal decision tree adaption by Domingos and Hulten~\cite{domingos00:vfdt} for streams named so-call Very Fast Decision Tree (VFDT), also commonly known as Hoeffding Tree (HT). Their method is based on the assumption that to find the best attribute for a split decision at any node in a decision tree, it would be sufficient to consider only a certain amount of training data that node. Hoeffding bound~\cite{hoeffding63:bound} is a measurement of degree of certainty of such approach, which gives an error bound for a decision taken after seeing a certain amount of instances. This bound essentially states that any decision taken after observing a certain amount of instances would remain the same after seeing an infinite number of instances and the error margin can be computed with this bound. Detailed discussion of this bound and the algorithm is presented in Chapter~\ref{chp:background}.

With such approach, challenges posed by high volume of data can be avoided. It does not require to remember all the observed data instances. Rather the statistics of the instances are sufficient to effectively decide about split attributes. One of the limitations of Hoeffding Tree is that it grows linearly as the time passes or instances arrives at the system. Root remains the node that was splitted by observing the oldest examples. Similarly, decision rules at lower levels also becomes very old, while the leaves contains the information from most recent instances. To keep the rules updated numerous approaches such as reseting the tree, pruning bad performing nodes, etc have been proposed. Detailed discussion of adaptations for concept drift, evolution, recurrence, etc. are being discussed in Chapter~\ref{chp:background}.

With reset and pruning facility Hoeffding Tree exhibits a special property. It always adapts itself for the newer examples. Number of examples HT's decision rules depend on is directly related to the size of the tree and number of examples required to split a node. Number of decision or split nodes could a measurement of the size of the tree. A smaller tree would adapt faster to the changes in the data. A larger tree would take longer time to adapt. Using this rationale a bagging method based of different sized trees is proposed by Bifet et al.~\cite{bifet09:asht}. In this method, a fixed number of Hoeffding Trees with different size (decision nodes) limits are used. Each time a tree exceeds the size threshold, the tree gets reset. Trivially, smaller trees would reset more often than the larger trees. And thus decision rules from smaller trees will based on the most recent data. Larger trees, however, would also have decision rules from older data.

We combined all these concepts to address our problem. First, we introduced a size restricted variant of Hoeffding Tree namely Size Restricted Hoeffding Tree (SRHT). Unlike its predecessor Adaptive Size Hoeffding Tree~\cite{bifet09:asht}, it does not get reset immediately after it reaches the size limit. Rather it stops growing i.e. no further split occurs, however, the weights in the leaf nodes are updated with incoming instances. It also combines two different concept introduces by its predecessors: (i) to reset once a size limit is reached and (ii) to maintain alternate trees or subtrees where necessary. Thus, even after reaching the size limit, a tree can switch to a alternate tree and start growing again till the limit is reached again. 

We use this setup for a modified bagging scheme introduced in~\cite{bifet09:asht}. Similar to the Hoeffding Tree, a smaller SRHT will have decision rules for most recent data, and a larger tree would contain rules for some older instances too. To put simply in our problem's context, smaller tree would have decisions for high speed sub-streams or burst of incoming streams. Whereas only the larger trees will have some decision rules for slow but consistent sub-streams, along with the decision rules for most recent streams. This is because, the recent data are always dominated by high speed sub-streams. Smaller does not get enough information about the slow stream or enough examples from slow streams to decide on them. Keeping this in mind, we control the reset of larger trees to keep hard-learned decision rules. In doing so, we maintain an alternate pool of trees that are to be reset soon, and start maintain a new tree with same size limit from scratch. For the transition time we consider votes from all trees, thus effectively increasing the weights for slower streams. In Chapter~\ref{}, we present more elaborate description of the algorithms. In Chapter~\ref{} we the  experimental evaluation where we compare the results obtain by our approach with the current state-of-the-art method. [summarize the resutls here]

\section{Contributions}
In summary, the main contributions of this thesis in the domain are following:

\begin{itemize}
    \item Detailed survey of the existing literature.
    \item As extensive experimental evaluation of decision tree based stream mining and ensemble approaches.
    \item A new look at the composition of various real-world streams e.g. social networks.
    \item Adaptation of Adaptive Size Hoeffding Tree into Size Restricted Hoeffding Tree.
    \item Combining the concepts of adaptation of concept by maintaining alternate tree and reseting of tree.
    \item Adaptation of bagging method to improve performance for slower streams, as well as to overcome limitation of ASHT.
    \item Adaptation of a data generation of approach to facilitate slow/fast, burst, recurrent behavior in a randomized data stream.
    \item Finally, presented justification of the new approach using real-world and synthesized data set.
\end{itemize}


\section{Outline}
This article is organized as follows: Chapter~\ref{chp:relworks} presents a broad survey of present literature. Chapter~\ref{chp:background} presents the base concepts required for the algorithm. ...
