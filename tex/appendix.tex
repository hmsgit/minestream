\begin{appendices}
\appendixpage
\noappendicestocpagenum
\addappheadtotoc

\chapter{List of Plots}

\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of drift coefficient on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:speed1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{1-rnd-speed-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{1-vs-speed-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of drift coefficient on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:speed2}
    \end{center}
\end{figure}






\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of grace period on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:grace1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{2-rnd-grace-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{2-vs-grace-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of grace period on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:grace2}
    \end{center}
\end{figure}

\clearpage





\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{3-rnd-centroid-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{3-vs-centroid-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{3-rnd-centroid-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{3-vs-centroid-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{3-rnd-centroid-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{3-vs-centroid-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of number of centroids on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:centroid1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{3-rnd-centroid-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{3-vs-centroid-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{3-rnd-centroid-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{3-vs-centroid-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{3-rnd-centroid-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{3-vs-centroid-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of number of centroids on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:centroid2}
    \end{center}
\end{figure}



\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{4-rnd-driftcentroid-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{4-vs-driftcentroid-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{4-rnd-driftcentroid-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{4-vs-driftcentroid-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{4-rnd-driftcentroid-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{4-vs-driftcentroid-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of percentage of drift centroids on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:driftcentroid1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{4-rnd-driftcentroid-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{4-vs-driftcentroid-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{4-rnd-driftcentroid-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{4-vs-driftcentroid-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{4-rnd-driftcentroid-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{4-vs-driftcentroid-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of percentage of drift centroids on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:driftcentroid2}
    \end{center}
\end{figure}

\clearpage


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{5-rnd-tiethresh-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{5-vs-tiethresh-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{5-rnd-tiethresh-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{5-vs-tiethresh-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{5-rnd-tiethresh-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{5-vs-tiethresh-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of tie threshold on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:tiethresh1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{5-rnd-tiethresh-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{5-vs-tiethresh-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{5-rnd-tiethresh-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{5-vs-tiethresh-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{5-rnd-tiethresh-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{5-vs-tiethresh-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of tie threshold on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:tiethresh2}
    \end{center}
\end{figure}



\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{6-rnd-binsplit-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{6-vs-binsplit-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{6-rnd-binsplit-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{6-vs-binsplit-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{6-rnd-binsplit-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{6-vs-binsplit-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of binary split centroids on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:binsplit1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{6-rnd-binsplit-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{6-vs-binsplit-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{6-rnd-binsplit-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{6-vs-binsplit-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{6-rnd-binsplit-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{6-vs-binsplit-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of binary split on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:binsplit2}
    \end{center}
\end{figure}

\clearpage


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{7-rnd-maxsize-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{7-vs-maxsize-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{7-rnd-maxsize-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{7-vs-maxsize-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{7-rnd-maxsize-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{7-vs-maxsize-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of maximum allowed size of tree on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:maxsize1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{7-rnd-maxsize-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{7-vs-maxsize-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{7-rnd-maxsize-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{7-vs-maxsize-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{7-rnd-maxsize-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{7-vs-maxsize-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of maximum allowed size of tree on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:maxsize2}
    \end{center}
\end{figure}



\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{8-vs-ensize-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{8-vs-ensize-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{8-vs-ensize-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of ensemble size on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:ensize1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{8-vs-ensize-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{8-vs-ensize-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{8-rnd-ensize-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{8-vs-ensize-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of ensemble size on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:ensize2}
    \end{center}
\end{figure}

\clearpage


\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{9-rnd-ifreset-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{9-vs-ifreset-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{9-rnd-ifreset-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{9-vs-ifreset-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{9-rnd-ifreset-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{9-vs-ifreset-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of tree reset (within ensemble) on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:ifreset1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{9-rnd-ifreset-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{9-vs-ifreset-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{9-rnd-ifreset-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{9-vs-ifreset-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{9-rnd-ifreset-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{9-vs-ifreset-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of tree reset (within ensemble) on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:ifreset2}
    \end{center}
\end{figure}



\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{10-rnd-firsttree-accu}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{10-vs-firsttree-accu}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{10-rnd-firsttree-time}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{10-vs-firsttree-time}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{10-rnd-firsttree-kappa}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{10-vs-firsttree-kappa}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of first tree size centroids on performance (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:firsttree1}
    \end{center}
\end{figure}
\begin{figure}[htbp] 
    \begin{center}
        \begin{tabular}{cc}
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{10-rnd-firsttree-depth}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{10-vs-firsttree-depth}.pdf}} \\
            \scriptsize{(a)} & \scriptsize{(d)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{10-rnd-firsttree-tsize}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{10-vs-firsttree-tsize}.pdf}} \\
            \scriptsize{(b)} & \scriptsize{(e)} \\
            
            \hspace{-5mm} \resizebox{80mm}{!}{\includegraphics{res/{10-rnd-firsttree-memory}.pdf}} &
            \hspace{-10mm} \resizebox{80mm}{!}{\includegraphics{res/{10-vs-firsttree-memory}.pdf}} \\
            \scriptsize{(c)} & \scriptsize{(f)} \\
            
        \end{tabular}
        \caption{Effect of first tree size on structure (a-c) random RBF (d-f) VS RBF}
        \label{fig:exp:effect:firsttree2}
    \end{center}
\end{figure}

\clearpage


\chapter{Traditional Learning Algorithms}
%\markboth{Appendix A}{Appendix A}
%\addcontentsline{toc}{chapter}{Appendix A. Basic Learning Algorithms}

\section*{Learning Algorithms}
\label{sec:bg:learningalgos}
The goal of data classification process is to predict an outcome based on some give data. In order to do so, data mining algorithms first processes a training set containing a set of attributes and corresponding outcome: a class or a value. Algorithms develop hypotheses that best describe the relationship among the attributes and the outcome for the total instance set.  Formally, learning problems are defined as follow: Given a data set of $m$ instances  $D \equiv \{ \{\vec{x}_1, y_1\}, \{\vec{x}_2, y_2\}, \{\vec{x}_3, y_3\}, \dots, \{\vec{x}_m, y_m\} \}$ where $ \vec{x} = \{x_1, x_2, x_3, \dots, x_n\}$, learning algorithms try to approximate $H \equiv y = f(\vec{x})$. Additional to being typical linear, polynomial, etc. functions, $H$ can also be a set of IF-THEN rules. These rules or functions are then used to predict unseen instances where outcome class is not known. These algorithms are known as learning algorithms, and in last few decades, have widely been researched in the field of machine learning. This section briefly discusses few of the basic algorithms upon which the foundation of ensemble methods and this thesis are laid. The detailed discussion of these algorithms are out of the scope of this chapter. Thus we discuss only the core concepts here.

\subsection*{$k$-Nearest Neighbors}
$k$-nearest neighbor algorithm, in short $k$-NN, is one the simplest learning available. It is a non-parametric instance-based lazy learning algorithm and works for both supervised and unsupervised learning. It uses similarity measure like distance functions to classify unknown instances. The decision is a majority voting of  $k$ neighbors where $k$ is predefined. For example, if $k= 1$ then an unknown instance is assigned the class label of its nearest neighbor. There are several schemes to find the $k$ nearest neighbors. For continuous data, some well-known distance functions are Euclidean distance, Manhattan distance, Minkowski distance, etc.  As in Figure~\ref{fig:bg:knn} 1-NN classification for the new instance is {--}, 2-NN is undefined, 3-NN is {+} using Euclidean distance.
\begin{itemize}
    \item Euclidean distance $D_E = \sqrt{\sum_{\i =1 }^{k} (x_i - y_i)^2}$
    \item Manhattan distance $D_M = \sum_{\i =1 }^{k} |(x_i - y_i)|$
    \item Minkowski distance $D_{Mink}\sqrt[q]{\sum_{\i =1 }^{k} (|x_i - y_i|)^q}$
\end{itemize}
For categorical data, Hamming distance can be used. Hamming distance is defined as $D_H = \sum_{\i =1 }^{k} |(x_i - y_i)|$ where $|x_i - y_i|$ is $0$ if $x=y$, $1$ otherwise.

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=3.0in]{figs/knn.pdf}
    \caption{Concept of $k$-NN}
    \label{fig:bg:knn}
\end{center}
\end{figure}

Choosing an optimal $k$ is the prime challenge of $k$-NN algorithm, which is extremely training data dependent. Changing position of few training points could lead to a significant loss in performance. The method is particularly not stable in the class boundary. $k$ should be large enough that $k$-NN could overcome the noises in data, and small enough that instances of other classes are not included. Generally, higher $k$ reduces the overall noise and should be more precise. For most data set a value between $3$-$10$ performs much better than $1$-NN. Typically cross-validation is used to determine a good $k$. 

\subsection*{Na\"ive Bayes}
Naive Bayes classification is based on Bayes rule. Bayes rule says that the probability of event $x$ conditioned on knowing event $y$, i.e. the probability of $x$ given $y$ is defined as
\[
p(x|y) = \frac{p(x,y)}{p(y)} = \frac{p(y|x) p(x)}{p(y)}
\]
The naive Bayes classifier~\cite{langley92:nb} assumes that all the explanatory variables are independent. Given a feature set $X = x_1, x_2, \dots, x_n$ of $n$ independent variables, naive Bayes assigns the instances to $k$ possible outcome or classes, $p(C_k | X)$. Using Bayes rule, the conditional probability can be decomposed as:
\[
p(C_k |X) = \frac{p(C_k) p(X|C_k)}{p(X)}.
\]
In other words,
\[
posterior = \frac{prior \times likelihood}{evidence}.
\]
Using chain rule repetitively, it can be expressed as follows:
\[
p(C_k |X) = p(C_k) \prod_{i=1}^n p(x_i | C_k)
\]
The predicted class is the one which maximizes the conditional probabilities $p(C_k|X)$, that is, classifier assigns the class label $\hat{y} = C_k$ for some $k$ that satisfies:
\[
\hat{y} = \underset{k \in \{1, \dots, K\}}{argmax}  p(C_k) \prod_{i=1}^n p(x_i | C_k)
\]

\subsection*{Decision Tree}
Decision trees (DTs) are another popular genre of non-parametric supervised learning algorithms. Trees allow a way to graphically organize a sequential decision process. Decision tree, a directed acyclic graph, contains decision nodes, each with branches for all alternate decisions. Leaf nodes are labeled with respective class label. Each path from root to leaf is a decision rule, consisting of conditional part (unions of all internal nodes' conditions) and a decision (of leaf node).

Decision trees use {\it information gain} to select the splitting attribute that would maximize the total entropy of each of the subtrees resulting from the spit. Entropy is a measurement of purity of an attribute, typically ranged between 0 and 1. A pure attribute, attribute with definitive value-class relationship, would have a low entropy and is easy to predict. On the other hand attribute with highly mixed value-class relationship would yield high entropy. A common entropy function is-
\[
entropy = - \sum_{i=1}^n p_i \lg p_i.
\]
where $p_1, p_2, \dots, p_n$ are the distributions of several class attributes and $\sum p_i = 1$. Information gain is the difference between old and new entropy after a split. This is good measurement of relevance of an attribute. However, in cases where an attribute can take on a large number of distinct values, information gain is not ideal. Presence of large set of values could uniquely identify the classes but also reduces chance to generalize unseen instance and should be avoided to be place near root.

Decision tree is, however, computationally feasible. Average cost of constructing a decision tree for $n$ instances with $m$ attributes is $O(mn \lg n)$, and querying time is $O(\lg n)$.

\subsection*{Neural Network}
The Neural Network (NN) is a learning method inspired by biological neural network. A set of inter-connected nodes (also known as perceptrons and neurons) mimics biological neural network. Figure~\ref{fig:bg:nnlayer} shows a skeleton of a neural network. Neural network is a weighted directed graph where a input layer is connected to the output layer via some layers of hidden layers. This is commonly known as Multi-Layer Perceptron (MLP). Decisions are made looking only into the output layer only. Hidden layers enlarge the space of hypotheses. If there is no hidden layer, then neural network becomes a simple regression problem i.e. output is a linear function of inputs. Each connection among these nodes has an associated weight. Given an input data set, these weights are updated accordingly to best fit the classification model. Learning is done by back-propagation algorithm~\cite{raul96:nn}. Errors are propagated back from the output layer to the hidden layer and weights are updated to minimize error.
\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=4.0cm]{figs/nnlayers.png}
    \caption{Neural network layers}
    \label{fig:bg:nnlayer}
\end{center}
\end{figure}

Learning starts with assigning a small value as initial weights to all the connection weights. In case of a single perceptron, learning process is analogous to moving a parametric hyperplane around. Let $w_i$ be the weight of $i$-th input, then after $t+1$ iteration weight $w_i(t+1) = w_i(t) + \nabla E_i(t)$, where $\nabla E_i$ is the gradient of the error function. For a input vector $\mathbf{x}$ and true output $y$, $E$ is defined as the squared error:
\[
E = \frac{1}{2} Err^2 = \frac{1}{2} (y - h_w(\mathbf{x}))^2
\]
Thus gradient of $E$ would be:
\begin{align*}
\nabla E = \frac{\partial E}{\partial W_j} & = Err \times \frac{\partial Err}{\partial W_j} = Err \times \frac{\partial}{\partial W_j} (y - g(\sum_{j=0}^n w_j x_j)) \\
& = - Err \times g^{\prime} (in) \times x_j
\end{align*}
Based on this, weight update rule would be:
\[
w_j = w_j + \alpha \times Err \times g^{\prime} (in) \times x_j
\]
where $\alpha$ is learning rate coefficient. Back propagation algorithm for multi-layer perceptron also works in similar fashion. Weights in the output layer are updated using following equation:
\[
w_{j,i} = w_{j,i} + \alpha \times a_j \times \nabla_i
\]
where $\nabla_i = Err \times g^\prime(in_i)$. Hidden layers propagates this error from the output layer using:
\begin{align*}
\nabla_j &= g^\prime (in_i) \sum_i w_{j,i} \nabla_i \\
w_{k, j} &= w_{k, j} + \alpha \times a_k \times \nabla_j
\end{align*}

MLP is a very expressive method. Only $1$ hidden layer is sufficient to represent all continuous functions and $2$ can represent any functions. MLP is prone to local minima, which is avoided by running MLP multiple times with different initial weight settings.


\end{appendices}